---
title: "Exponential Smoothing: Part 2"
author: "Timo Meiendresch"
date: "31/05/2019"
knit: (function(input_file, encoding) {
  out_dir <- 'html_outputs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'FPP_7_ES2.html'))})
---

```{r, message=FALSE, echo=FALSE}
rm(list=ls())
graphics.off()
```

```{r, message=FALSE}
# libraries
library("fpp2")
```

#### Example: Holt-Winter (additive / multiplicative)

```{r}
# data
aust <- window(austourists,start=2005)

# holt-winter
fit1 <- hw(aust,seasonal="additive")
fit1[["model"]]
fit2 <- hw(aust,seasonal="multiplicative")
fit2[["model"]]

# plot
autoplot(aust) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Visitor nights (millions)") +
  ggtitle("International visitors nights in Australia") +
  guides(colour=guide_legend(title="Forecast"))


```

- Smoothing parameters and initial estimates for components have been estimated by minimising RMSE

#### Holt-Winters' damped method

Damped trend is possible with additive and multiplicative seasonality. Often accurate and robsut forecasts for seasonal data with a damped trend and multiplicative seasonality, i.e. using `hw(y, damped=TRUE, seasonal="multiplicative")`:

$$
\begin{align*}
  \hat{y}_{t+h|t} &= \left[\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}\right]s_{t+h-m(k+1)}. \\
  \ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}             \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}
$$

```{r}
### Example: HW daily data
# model
fc <- hw(subset(hyndsight,end=length(hyndsight)-35),
         damped = TRUE, seasonal="multiplicative", h=35)

# plot
autoplot(hyndsight) +
  autolayer(fc, series="HW multi damped", PI=FALSE)+
  guides(colour=guide_legend(title="Daily forecasts"))
```

## Taxonomy of exponential smoothing

Variations in the components of the trend and seasonal components yield nine exponential smoothing methods. 

![](images/Classification_ES.png)

Some of these have already been introduced before:

![](images/Shorthand.png){width=66%}

Multiplicative trend methods are not considered here as they tend to produce bad forecasts. 

Overview of all the equations related to the nine exponential smoothing methods: 

![](images/pegelstable.png)

- $l_t$ denotes series level
- $b_t$ denotes slope at time t
- $s_t$ denotes the seasonal component of the series
- $m$ denotes number of seasons in a year
- $\alpha, \beta^\star, \gamma, \phi$ are smoothing parameter
- $\phi_h = \phi + \phi^2 + ... + \phi^h$
- $k$ is the integer part of $(h-1)/m$

## Innvations state space models for exponential smoothing

So far, algorithms which generate point forecasts. Now, we'll use a statistical model which also generates prediction intervals. 

- **Statistical Model:** Stochastic, data-generating process

Each model consists of: 

- Measurement equation: describing observed data
- State equations: describe unobserved components or states (level, trend, seasonal) changing over time

Both equations together are referred to as **state space models**. Each method has two models: one with additive errors and one with multiplicative errors. Point forecasts produced are identical if they have the same smoothing parameter values. 

To distinguish between model with additive/multiplicative errors, a third letter is added leading to the model called ETS(.,.,.) for (Error, Trend, Seasonal). 

Possibilities for each component: 

- $Error = {A, M}$
- $Trend = {N, A, A_d}$
- $Seasonal = {N, A, M}$

#### ETS(A,N,N): Simple exponential smoothing, additive errors 

Recall the component form: 

$$
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+1|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
$$

Re-arrange smoothing equation for the level

$$
\begin{align*}
\ell_{t} &= \alpha y_{t}+\ell_{t-1}-\alpha\ell_{t-1}\\
         &= \ell_{t-1}+\alpha( y_{t}-\ell_{t-1})\\
         &= \ell_{t-1}+\alpha e_{t}
\end{align*}
$$

where 

$$
e_t = y_t - l_{t-1} = y_t - \hat{y}_{t|t-1}
$$

We can also rewrite this as 

$$ 
y_t = l_{t-1} + e_t 
$$
- Each observation is represented by last period's level plus error
- Specify probability distribution for $e_t$ to make it an innovations state space model
- Additive errors: Assume residuals (one-step training errors) $e_t$ are normally distirbuted. 

$$
\begin{align}
  y_t &= \ell_{t-1} + \varepsilon_t \\
  \ell_t&=\ell_{t-1}+\alpha \varepsilon_t. 
\end{align}
$$

Where the first is the measurement (or observation) equation and the second the state equation. Togehter with the assumption of the error distribution we have a fully specified statistical model. 

- Measurement equation shows relationship between observations and unobserved states. 
- Here: $y_t$ is a linear function of the predictable part (the level $l_t$) and the unpredictable part (error $\varepsilon_t$)
- State equation shows the evolution of the state


#### ETS(M, N, N): Simple exponential smoothing with multiplicative errors

Specify models with mult





