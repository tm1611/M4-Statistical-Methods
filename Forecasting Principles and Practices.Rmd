---
title: 'Forecasting: Principles and Practice'
author: "Timo Meiendresch"
date: "22/05/2019"
knit: (function(input_file, encoding) {
  out_dir <- 'html_outputs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'Forecasting_PP.html'))})
---

```{r, message=FALSE, echo=FALSE}
rm(list=ls())
graphics.off()
```

```{r, message=FALSE}
# libraries
library("ggplot2")
library("forecast")
library("fpp2")
library("GGally")
```

# Introduction
The following document contains my own notes of the book

> Hyndman, R. J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia.  

Notes are taken with regard to what is important to me and my own objectives. Therefore some chapters are not fully covered, others are excluded entirely. 

# Chapter 1: Getting started

- Book focuses on reliable methods for producing forecasts with emphasis on replicable and testable methods that have been shown to work.

Predictability of events or quantities depends on various factors, which can be addressed with the following three questions:

- how well do we understand the factors that contribute to the realization?
- how much data is available?
- can the forecast affect the thing we are trying to forecast?

In general, there are clear limits of what a forecast can achieve and one should be aware of its limitations. Also, environments are changing constantly and some of those changes are not reflected in the data. However, forecasting can assist in detecting the characteristics of the environment that is changing which is helpful as well.

Different forecasting situations according to

- time horizons
- factors determining outcomes
- types of data patterns

require different forecasting techniques. Choice of methods depends on 

- data availability
- predictability of the quantitiy to be forecast.

#### Forecasting, planning and goals

- **Forecasting:** Predicting the future as accurately as possible, given all of the information available, including historical data and knowledge of any future events that might impact the forecasts.
- **Goals:** What you would like to happen. Goals should be linked to forecasts and plans.
- **Planning:** Response to forecasts and goals. Determining the appropriate actions that are required to make your forecasts match your goals.

Difference in time horizon of the forecast into short-term forecasts, medium-term forecasts, and long-term forecasts

Appropriate forecasting method largely depends on what data are available. 

- No data: qualitative forecasting methods 

Conditions for quantitative forecasting: 

1. Numerical information about the past is available
2. Reasonable to assume that some aspects of the past patterns will continue into the future.

#### Time series forecasting
Anything that is observed sequentially over time is a time series. Here we will focus on time series that are observed at regular intervals over time. The objective is to estimate how the sequence of observations will continue into the future. The simplest time series forecasting methods use only information on the variable to be forecasted and make no attempt to discover the factors that affect its behaviour.

### Predictor variables and time series forecasting
Model for electricity demand with predictor variables might be of the form: 
$$ED = f(current\:temperature, \:strength\:of\:economy,\:population, \\ \:time\:of\:day,\:day\:of\:week,\:error)  $$

The relationship is not exact as there will always be changes that cannot be accounted for by the predictor variables.

A suitable time series equation for this problem without predictors may be of the form 

$$ED_{t+1} = f(ED_t, ED_{t-1}, ...., error)$$

A third type of model combines the features of the above two models. It may be of the form

$$ED_{t+1} = f(ED_t, \: current \: temperature, \: time \: of \: day,\: day \: of \: week,\: error$$

which is a type of "mixed model". They are known as dynamic regression models, panel data models, longitudinal models, transfer function models, and linear system models (if f is linear).

Including predictor variables is useful if we want to incorporate those information but there reasons a forecaster might select a time series model rather than an explanatory or mixed model: 

1. System may not be understood or relationship is difficult to capture
2. Necessary to know or forecast future values of the various predictors
3. Main concern may be to predict what will happen, not to know why it happens.
4. Time series model may give more accurate forecasts than an explanatory or mixed model. 

### Basic steps in a forecasting task
Usually involves five basic steps

Step 1: Problem definition

- Define problem, objectives, who requires the forecast, how forecasting function fits within the organisation
- Check needs and requirements with everyone who will be involved in collecting data, maintaining databases, and using the forecasts for future planning

Step 2: Gathering information

- Statistical data
- Cumulated expertise

Step 3: Preliminary (exploratory) analysis

- Always start with graphing the data 
- Consistent patterns like trend, seasonalities, cycles, outliers?

Step 4: Choosing and fitting models

- Best model depends on the availability of historical data, strength of relationships (predictor variables and forecast variable)
- Common to compare various models

Step 5: Using and evaluating a forecasting model

- Assess accuracy of forecasts.

#### Statistical perspective

Realization of forecast is unknown at the time of the forecast. Hence, we think of it as a random variable. In most contexts, variation associated with forecast will shrink as event approaches. 

When obtaining a forecast, we are estimating the middle of the range of possible values of the random variable. The prediction interval gives a range of values the random variable could take with high probability. The set of values that the random variable could take, along with their relative probabilities, is known as the probability distribution, named **forecast distribution**.

The "forecast" is the mean - or sometimes median - of the forecast distribution, indicated as $\hat{y}$. 

# Chapter 2: Time series graphics

- First thing to do: Plot the data

A time series can be thought of as a list of numbers along with information about what time those numbers were recorded.

- Store data as `ts()` object in R

```{r}
nums <- c(123,39,78, 52,110)
y <- ts(nums, start=2012)

# frequency?
y <- ts(nums, start=2012, frequency=4)
```

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results="asis"}
tabl <- "
| Data    | frequency |
|---------|-----------|
| Annual  | 1         |
| Quarterly | 4       |
| Monthly | 12        |
| Weekly  | 52        | 
"
cat(tabl)
```

```{r}
autoplot(melsyd[,"Economy.Class"]) +
  ggtitle("Economy class passengers: Melbourne-Sydney") +
  xlab("Year") +
  ylab("Thousands")
```

Features: 

- no passengers carried in 1989 due to a dispute
- reduced load due to a trial in 1992
- ...

These features have to be detected and accounted for.

```{r}
autoplot(a10) +
  ggtitle("Antidiabetic drug sales") +
  ylab("$ million") +
  xlab("Year")

```

- clear trend
- seasonality (increasing in magnitude)

#### Time series patterns

- Trend: long-term increase or decrease in the data. Might also be changing over time.
- Seasonal: Pattern with fixed and known frequency
- Cyclic: Rises and falls which are not of a fixed frequency

```{r}
# visualize seasonal pattern
ggseasonplot(a10, year.labels = TRUE, year.labels.left=TRUE) + 
  ylab("$ million") + 
  ggtitle("Seasonal plot: antidiabetic drug sales")

# alternatively
ggseasonplot(a10, polar=TRUE) +
  ylab("$ million") +
  ggtitle("Polar seasonal plot: antidiabetic drug sales")

# emphasise seasonal patterns
ggsubseriesplot(a10) +
  ylab("$ million") +
  ggtitle("Seasonal subseries plot: antidiabetic drug sales")

```

#### Scatterplots

```{r}
autoplot(elecdemand[,c("Demand","Temperature")], facets=TRUE) +
  xlab("Year: 2014") + ylab("") +
  ggtitle("Half-hourly electricity demand: Victoria, Australia")

# relationship demand and temperature
qplot(Temperature, Demand, data=as.data.frame(elecdemand)) +
  ylab("Demand (GW)") + xlab("Temperature (Celsius)")

```

Also, it is common to compute **correlations**, i.e. the correlation coefficient r. The correlation coefficient measures only the linear relationship.

### Scatterplot matrics

```{r}
autoplot(visnights[,1:5], facets=TRUE) +
  ylab("Number of visitor nights each quarter (millions)")

# check all relationships
#GGally::ggpairs(as.data.frame(visnights[,1:5]))
```

#### Lagged plots

```{r}
beer2 <- window(ausbeer, start=1992)
gglagplot(beer2)
```

#### Autocorrelation
Correlation measures the extent of a linear relationship between two variables, atuocorrealtion measures the linear realtionship between lagged values of a time series. 

```{r}
# correlogramm
ggAcf(beer2)

```

Effects on ACF:

- Trend: ACF for small lags tend to be large and positive. ACF tend to have positive values that slowly decrease as the lags increas
- Seasonal: ACF larger for seasonal lags (multiples of seasonal frequency)
- Trend and seasonal: Combination of these effects

```{r}
aelec <- window(elec, start=1980)
autoplot(aelec) + 
  xlab("Year") +
  ylab("GWh")

ggAcf(aelec, lag=48)

```


### White noise
Time Series that show no autocorrelation are called white noise. Sum of white noise realizations is called random walk

```{r}
set.seed(30)
y <- ts(rnorm(50))
autoplot(y) +
  ggtitle("White Noise")

ggAcf(y)

```

#### Exercises

```{r}
# gold
autoplot(gold)
frequency(gold)
gold[which.max(gold)]

# read in the data
tute1 <- read.csv("data/tute1.csv", header=TRUE)

# convert to time series 
my_ts <- ts(tute1[,-1], start=1981, frequency=4)

autoplot(my_ts, facets=T)

# retail
retail <- readxl::read_excel("data/retail.xlsx", skip=1)
my_ts2 <- ts(retail[,"A3349873A"],
             frequency=12,
             start=c(1982,4))

autoplot(my_ts2)
ggseasonplot(my_ts2)
ggseasonplot(my_ts2, polar = TRUE)
ggsubseriesplot(my_ts2)
gglagplot(my_ts2)
ggAcf(my_ts2)

dj_d <- diff(dj)
autoplot(dj_d)
ggAcf(dj_d)
```

# Chapter 3: The forecaster's toolbox

#### Average method
Forecats of all future values are equal to the average of historical data

$$ \hat{y}_{T+h|T} = \bar{y} $$

```{r}
# fc for white noise
fc_y <- meanf(y, h=10)
autoplot(fc_y)
```

#### Naive method
forecast is simply the value of the last observation

$$ \hat{y}_{T+h|T} = y_T $$
This method works surprisingly well for many economic and financial time series

```{r}
fc2_y <- naive(y,h=10)
autoplot(fc2_y)
```

#### Seasonal naive method
Set each forecast to be equal to the last observed value from the same season 

$$\hat{y}_{T+h|T} = y_{T+h-m (k+1)}$$
where m is the seasonal period, and k is the integer part of $(h-1)/m$

```{r}
fc_my_ts2 <- snaive(my_ts2, h=10)
```

#### Drift method
Variation of naive method to allow the forecast to increase or decrease over time. Equivalent to drawing a line between the first and last observation, and extrapolating it into the future


#### Examples
```{r}
# Plot some forecasts
autoplot(beer2) +
  autolayer(meanf(beer2, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(beer2, h=11),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(beer2, h=11),
    series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))

# non-seasonal methods
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```

Often, these forecasting methods serve as benchmarks rather than the method of choice.

## Transformations and adjustments
Use transformations and adjustmens to simplify the patterns in the historical data by removing known sources of variation or making the pattern more consistens across the whole data set. 

#### Calendar adjustments

```{r}
# Problem: different days of month
dframe <- cbind(Monthly=milk,
                DailyAverage = milk/monthdays(milk))
autoplot(dframe, facet=T) +
  xlab("Years") + 
  ylab("Pounds") +
  ggtitle("Milk production per cow")

```

#### Population adjustments
Adjust data to give per-capita data. Easier to interpret and compare and eliminates the effect of population changes.

#### Inflation adjustments
Use a price index to account for changes of purchasing power. With price index $z_t$ and $y_t$ denotes the original house price in year t,

$$ x_t = y_t / z_t \cdot z_{year} $$

A common price index is the Consumer Price Index (CPI).

#### Mathematical transformations
Variation that increases or decreases with the level of that series can be approached with transformations like the log-transformation. Other transformations: 

- power transformations
- Box-Cox transformations

```{r}
lambda <- BoxCox.lambda(elec)
autoplot(BoxCox(elec, lambda))
```

Afterwards, the inverse of the Box.Cox transformation (`InvBoxCox()`) should be applied to obtain the initial scale. 

#### Bias adjustments
Back-transformed point forecast such as Box-Cox transformation will usually be the median of the forecast distritbution (assuming that the distibribution on the transformed space is symmetric). If you want to add up different forecasts and combine them this should be adjusted (bias-adjusted). Medians do not add up, whereas means do.

```{r}
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation") +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
```

If you want your forecats to be means rather than medians, use the argument `biasadj=TRUE` when you select your Box-Cox transformation parameter.

## Residual diagnostics
Residuals are equal to the difference between the observations and the corresponding fitted values: 

$$e_t = y_t - \hat{y}_t$$
They are useful to check whether a model has captured the information in the data (only white noise should be left in residuals). A good forecasting method yields residuals with the following properties:

- Residuals are uncorrelated. 
- Residuals have zero mean. 

In addition, it is useful but not necessary that: 

- Residuals have constant variance
- Residuals are normally distributed.

The latter properties make the calculation of prediction intervals easier. Alternative approach to obtain prediction intervals may be necessary. 

```{r}
autoplot(goog200) +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google Stock (daily ending 6 December 2013)")

# residuals 
res <- residuals(naive(goog200))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")

# Histogram
gghistogram(res) + ggtitle("Histogram of residuals")

ggAcf(res) + ggtitle("ACF of residuals")
```

#### Portmanteau test for autocorrelation
A test for a group of autocorrelations is called a portmanteau test. Common tests are the **Box-Pierce Test** and the **Ljung-Box test**. 
These are tesing whether the first h autocorrelations are significantly different from what would be expected from a white nois process. 

Not significant results mean they are not distinguishable from a white noise process. Use the function `checkresiduals()`to produce a time plot of the residuals, ACF, histogram and a Ljung-Box test altogether.

## Evaluating forecast accuracy
Accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model.

- Common practice to separate the available data into two portions, **training** and **test** data, where the training data is used to estimate any parameters of a forecasting method and the test data is used to evaluate its accuracy. Size of the test data is typically about 20% of the total sample.

Note:

- model which fis the training data well is not necessarily a good forecasting model
- Perfect fit can always be obtained by using a model with enough parameters.
- Over-fitting a model is just as bad as failing systematic patterns in the data.

#### Functions to subset a time series
```{r}
# window
my_beer <- window(ausbeer, start=1995)
str(ausbeer)
str(my_beer)

# subset
my_beer2 <- subset(ausbeer, start=length(ausbeer)-4*5)
str(my_beer2)

#
my_beer3 <- subset(ausbeer, quarter=1)
head(my_beer3)
```

#### Forecast errors
Difference between actual realization and its forecast. Error may mean unpredictable part of an observation

$$e_{T+h} = y_{T+h} - \hat{y}_{T+h|T}$$

Different from residuals because:

- Residuals are calculated on the training set while forecast errors are calculated on the test set. 
- Residuals are based on one-step forecasts while forecast errors can involve multi-step forecasts

#### Scale-dependent errors
scale-dependent errors can not be used to make comparisons between series of different units.

Common scale-dependent measures:

- Mean absolute error (MAE): $MAE=mean(|e_t|)$
- Root mean squared error (RMSE): $\sqrt{mean(e_t^2)}$

FOrecast method minimising the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. 

#### Percentage errors
Advantage of being unit-free

- Percentage error: $p_t = 100 \times (e_t / y_t)$
- Mean absolute percentage error (MAPE): $MAPE = mean(|p_t|)$

Those measures based on percentage errors have the disadvantage of being infinite or undefined if $y_t = 0$ for any t in the period of interest, and having extreme values if any $y_t$ is close to zero. 

Another commonly used measure is the sMAPE (used in M3 for example). Hyndman & Koehler (2006) recommend that the sMAPE not be used. 

#### Scaled errors 
In the same paper scaled errors were proposed as alternative when **comparing forecast accuracy across series with different units**. 

- How to: Scaling errors based on the *training MAE* from a simple forecast method.

For a non-sseasonal time series, useful way to define a scaled error uses naive forecasts:

$$q_j = \frac{e_j}{\frac{1}{T-1} \sum_{t=2}^T |y_t - y_{t-1}|}$$
where $y_t - y_{t-1}$ is the naive forecast error.

- Numerator and denominator both involve values on the scale of the original data, hence $q_j$ is independent of the scale of the data. 
- Scaled error is less than one if it arises from a better forecast than the average naive forecast computed on the training data. 
- It is greater than one if the forecast is worse than the average naive forecast computed on the training data. 

For seasonal time series, a scaled error can be defined using seasonal naive forecasts:

$$q_j = \frac{e_j}{\frac{1}{T-m} \sum_{t=m+1}^T |y_t - y_{t-m}|}$$

and the mean absolute scaled error is simply:

$$MASE = mean(|q_j|)$$

```{r}
# Seasonal Example
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
beerfit1 <- meanf(beer2,h=10)
beerfit2 <- rwf(beer2,h=10)
beerfit3 <- snaive(beer2,h=10)
autoplot(window(ausbeer, start=1992)) +
  autolayer(beerfit1, series="Mean", PI=FALSE) +
  autolayer(beerfit2, series="Naïve", PI=FALSE) +
  autolayer(beerfit3, series="Seasonal naïve", PI=FALSE) +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") +
  guides(colour=guide_legend(title="Forecast"))

# accuracy
beer3 <- window(ausbeer, start=2008)
accuracy(beerfit1, beer3)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(beerfit2, beer3)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(beerfit3, beer3)[,c("RMSE","MAE", "MAPE","MASE")]
```

For now, the seasonal naive method is best for these data (from graph and measures). 

Nonseasonal example:

```{r}
googfc1 <- meanf(goog200, h=40)
googfc2 <- rwf(goog200, h=40)
googfc3 <- rwf(goog200, drift=TRUE, h=40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, PI=FALSE, series="Mean") +
  autolayer(googfc2, PI=FALSE, series="Naïve") +
  autolayer(googfc3, PI=FALSE, series="Drift") +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))

# accuracy
googtest <- window(goog, start=201, end=240)
accuracy(googfc1, googtest)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(googfc2, googtest)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(googfc3, googtest)[,c("RMSE","MAE", "MAPE","MASE")]
```

Here, the drift method seems to be the best among the three forecasts. 

## Time series cross-validation
Series of test sets, each consisting of a single observation. The corresponding training set consists only of observations that occurred *prior* to the observation that forms the test set. Forecast accuracy is computed by averaging over the test sets. 

- Also known as "evaluation on a rolling forecasting origin" because the "origin" at which the forecast is based rolls forward in time.

Cross-validation procedure can be modified to allow multi-step errors to be used (suppose we are interested in models that produce for example good 4-step-ahead forecasts).  

Time series cross-validation is implemented with the `tsCV()` function. 

```{r}
# Compare RMSE of time series cross validation with residual RMSE
e <- tsCV(goog200, rwf, drift = TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))
```

- RMSE from residuals is smaller, as the corresponding "forecasts" are based on a model fitted to the entire data set, rather than being true forecasts.
- Good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation.


#### Pipe operator

```{r}
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e

e^2 %>% 
  mean(na.rm=TRUE) %>% 
  sqrt()

goog200 %>% rwf(drift=TRUE) %>% residuals() -> res

res^2 %>% 
  mean(na.rm=TRUE) %>% 
  sqrt()

```

Note that with pipes it may seem more natural to use the right arrow assignment `->` rather than the left arrow.

```{r}
# Example: using tsCV()
e <- tsCV(goog200, forecastfunction = naive, h=8)

# Compute MSE and remove missing values
mse <- colMeans(e^2, na.rm=TRUE)

# plot mse against forecast horizon
data.frame(h=1:8, MSE = mse) %>% 
  ggplot(aes(x=h, y=MSE)) +
  geom_point()
```

## Prediction Intervals 
PI givbes an interval within which we expect $y_t$ to lie with a specified probability. 

Assuming the forecast errors are normally distributed, a 95% prediction interval for the h-step forecast is given by 
$$ \hat{y}_{T+h|T} \, \pm 1.96 \hat{\sigma}_h $$

where $\hat{\sigma}_h$ is an estimate of the standard deviation of the h-step forecast distribution. 

If we only produce point forecasts, there is no way of telling how accurate the forecasts are. However, if we also produce prediction intervals, then it is  clear how much uncertainty is associated with each forecast. 

#### One-step prediction intervals
Almost the same as the standard deviation of the residuals for one step ahead predictions. 

- No parameters (naive method): standard deviation is the same for forecast and residual

#### Multi-step prediction intervals
- PI usually increases as forecast horizon increases.
- Multi-step forecasts require a more complicatred method

#### Benchmark methods
Four benchmark methods to mathematically derive the forecast standard devaition under the assumption of uncorrelated residuals. 


#### Prediction intervals from bootstrapped residuals
Assumption that forecst errors are normally distributed:

- Rewrite: $y_t = \hat{y}_{t|t-1}+ e_t$
- Simulate: $y_{T+1} = \hat{y}_{T+1|T} + e_{T+1}$

where $\hat{y}_{T+1|T}$ is the one-step forecast and $e_{T+1}$ the unknown future error. Assuming future errors will be similar to past errors, we replace these errors by sampling from the colleciton of errors we have seen in the past (i.e. residuals). In this way, we can simulate an entire set of future valus for our time series. 

- Repeat: Doing this repeatedly, we obtain many possible futures from which we can computed prediction intervals by calculating percentiles for each forecast horizong.

```{r}
naive(goog200, bootstrap=TRUE, h=5)
```

If transformation has been used, prediction interval should be computed on the transformed scale, and the end points back-transformed to give a prediction interval on the original scale. Back-transforamtion is done autocamitally using the functions in the **forecast** package.

## The forecast package in R
Many functions of this package produce output of class `forecast`. This object contains information about forecasting method, data used, point forecasts obtained, PI, residauls and fitted values. 

Functions designed to work with these objects: 

- `autoplot()`
- `summary()`
- `print()`

List of all functions that produce `forecast`objects: 

- `meanf()`
- `naive(), snaive()`
- `rwf()`
- `croston()`
- `stlf() `
- `ses()`
- `holt(), hw()`
- `splinef()`
- `thetaf()`
- `forecast()`

#### `forecast()` function
We have used functions which produce a `forecast()`object directly. More common approach:

- fit a model to the data
- use the `forecast()` function to produce forecats

Uses automatic ETS algorithm if first argument is of class `ts` (hence if it is not already a forecast model).

```{r}
# blind model
forecast(ausbeer, h=4)
```


## Exercises

1. Find an appropriate Box-Cox transformation in order to stabilise the variance

```{r}
# 1. Find appropriate Box-Cox transformation 
autoplot(mcopper)
lmd <-BoxCox.lambda(mcopper)
autoplot(BoxCox(mcopper, lmd))

# 2. Why BC unhelpful for cangas
autoplot(cangas)
lmd <- BoxCox.lambda(cangas)
autoplot(BoxCox(cangas, lmd))
```

As variance increases and then decreases there is no BoxCox transformation that can mimic this changing behavior of the series.

```{r}
# 5 Calculate residuals
beer <- window(ausbeer, start=1992)
fc <- snaive(beer)
autoplot(fc)
res <- residuals(fc)
autoplot(res)
#checkresiduals(res)
```






```{r}
# 8
myts.train <- window(my_ts2, end = c(2010, 12))
myts.test <- window(my_ts2, start = 2011)

autoplot(my_ts2) + 
  autolayer(myts.train, series ="Training") +
  autolayer(myts.test, series = "Test")

fc_snaive <- snaive(myts.train)
fc_arima <- forecast(auto.arima(myts.train))
fc_ets <- forecast(ets(myts.train))
fc_tbats <- forecast(tbats(myts.train))

fc_comb1 <- 0.5*(fc_arima$mean + fc_ets$mean)
fc_comb2 <- (1/3)*(fc_snaive$mean + fc_arima$mean + fc_ets$mean)
fc_comb3 <- (1/3)*(fc_snaive$mean + fc_ets$mean + fc_tbats$mean)

accuracy(fc_snaive, myts.test)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(fc_arima, myts.test)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(fc_ets, myts.test)[,c("RMSE","MAE", "MAPE","MASE")]
accuracy(fc_tbats, myts.test)[,c("RMSE","MAE", "MAPE","MASE")]

accuracy(fc_comb1, myts.test)[,c("RMSE","MAE", "MAPE")]
accuracy(fc_comb2, myts.test)[,c("RMSE","MAE", "MAPE")]
accuracy(fc_comb3, myts.test)[,c("RMSE", "MAE", "MAPE")]

checkresiduals(fc_snaive)
```

```{r}
# 9.
train1 <- window(visnights[, "QLDMetro"], end=c(2015,4))
train2 <- window(visnights[, "QLDMetro"], end=c(2014,4))
train3 <- window(visnights[, "QLDMetro"], end=c(2013,4))

# fc
fc1 <- forecast(snaive(train1), h=4)
fc2 <- forecast(snaive(train2), h=4)
fc3 <- forecast(snaive(train3), h=4)

autoplot(window(visnights[,"QLDMetro"], start=2014)) +
  autolayer(fc1, PI=FALSE, series="Forecast 1") +
  autolayer(fc2, PI=FALSE, series="Forecast 2") +
  autolayer(fc3, PI=FALSE, series="Forecast 3")

accuracy(fc1, visnights[,"QLDMetro"])[,"MAPE"]
accuracy(fc2, train1)[,"MAPE"]
accuracy(fc3, train2)[,"MAPE"]
```

```{r}
# 10. 
autoplot(dowjones)

# drift method
fc_drift <- forecast(rwf(dowjones, drift = TRUE))
autoplot(fc_drift)
```

```{r}
# 11.

```


