---
title: 'Forecasting: Principles and Practice'
author: "Timo Meiendresch"
date: "22/05/2019"
knit: (function(input_file, encoding) {
  out_dir <- 'html_outputs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'Forecasting_PP.html'))})
---

```{r, message=FALSE, echo=FALSE}
rm(list=ls())
graphics.off()
```

```{r, message=FALSE}
# libraries
library("ggplot2")
library("forecast")
library("fpp2")
library("GGally")
```

# Introduction
The following document contains my own notes of the book

> Hyndman, R. J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia.  

Notes are taken with regard to what is important to me and my own objectives. Therefore some chapters are not fully covered, others are excluded entirely. 

# Chapter 1: Getting started

- Book focuses on reliable methods for producing forecasts with emphasis on replicable and testable methods that have been shown to work.

Predictability of events or quantities depends on various factors, which can be addressed with the following three questions:

- how well do we understand the factors that contribute to the realization?
- how much data is available?
- can the forecast affect the thing we are trying to forecast?

In general, there are clear limits of what a forecast can achieve and one should be aware of its limitations. Also, environments are changing constantly and some of those changes are not reflected in the data. However, forecasting can assist in detecting the characteristics of the environment that is changing which is helpful as well.

Different forecasting situations according to

- time horizons
- factors determining outcomes
- types of data patterns

require different forecasting techniques. Choice of methods depends on 

- data availability
- predictability of the quantitiy to be forecast.

#### Forecasting, planning and goals

- **Forecasting:** Predicting the future as accurately as possible, given all of the information available, including historical data and knowledge of any future events that might impact the forecasts.
- **Goals:** What you would like to happen. Goals should be linked to forecasts and plans.
- **Planning:** Response to forecasts and goals. Determining the appropriate actions that are required to make your forecasts match your goals.

Difference in time horizon of the forecast into short-term forecasts, medium-term forecasts, and long-term forecasts

Appropriate forecasting method largely depends on what data are available. 

- No data: qualitative forecasting methods 

Conditions for quantitative forecasting: 

1. Numerical information about the past is available
2. Reasonable to assume that some aspects of the past patterns will continue into the future.

#### Time series forecasting
Anything that is observed sequentially over time is a time series. Here we will focus on time series that are observed at regular intervals over time. The objective is to estimate how the sequence of observations will continue into the future. The simplest time series forecasting methods use only information on the variable to be forecasted and make no attempt to discover the factors that affect its behaviour.

### Predictor variables and time series forecasting
Model for electricity demand with predictor variables might be of the form: 
$$ED = f(current\:temperature, \:strength\:of\:economy,\:population, \\ \:time\:of\:day,\:day\:of\:week,\:error)  $$

The relationship is not exact as there will always be changes that cannot be accounted for by the predictor variables.

A suitable time series equation for this problem without predictors may be of the form 

$$ED_{t+1} = f(ED_t, ED_{t-1}, ...., error)$$

A third type of model combines the features of the above two models. It may be of the form

$$ED_{t+1} = f(ED_t, \: current \: temperature, \: time \: of \: day,\: day \: of \: week,\: error$$

which is a type of "mixed model". They are known as dynamic regression models, panel data models, longitudinal models, transfer function models, and linear system models (if f is linear).

Including predictor variables is useful if we want to incorporate those information but there reasons a forecaster might select a time series model rather than an explanatory or mixed model: 

1. System may not be understood or relationship is difficult to capture
2. Necessary to know or forecast future values of the various predictors
3. Main concern may be to predict what will happen, not to know why it happens.
4. Time series model may give more accurate forecasts than an explanatory or mixed model. 

### Basic steps in a forecasting task
Usually involves five basic steps

Step 1: Problem definition

- Define problem, objectives, who requires the forecast, how forecasting function fits within the organisation
- Check needs and requirements with everyone who will be involved in collecting data, maintaining databases, and using the forecasts for future planning

Step 2: Gathering information

- Statistical data
- Cumulated expertise

Step 3: Preliminary (exploratory) analysis

- Always start with graphing the data 
- Consistent patterns like trend, seasonalities, cycles, outliers?

Step 4: Choosing and fitting models

- Best model depends on the availability of historical data, strength of relationships (predictor variables and forecast variable)
- Common to compare various models

Step 5: Using and evaluating a forecasting model

- Assess accuracy of forecasts.

#### Statistical perspective

Realization of forecast is unknown at the time of the forecast. Hence, we think of it as a random variable. In most contexts, variation associated with forecast will shrink as event approaches. 

When obtaining a forecast, we are estimating the middle of the range of possible values of the random variable. The prediction interval gives a range of values the random variable could take with high probability. The set of values that the random variable could take, along with their relative probabilities, is known as the probability distribution, named **forecast distribution**.

The "forecast" is the mean - or sometimes median - of the forecast distribution, indicated as $\hat{y}$. 

# Chapter 2: Time series graphics

- First thing to do: Plot the data

A time series can be thought of as a list of numbers along with information about what time those numbers were recorded.

- Store data as `ts()` object in R

```{r}
nums <- c(123,39,78, 52,110)
y <- ts(nums, start=2012)

# frequency?
y <- ts(nums, start=2012, frequency=4)
```

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results="asis"}
tabl <- "
| Data    | frequency |
|---------|-----------|
| Annual  | 1         |
| Quarterly | 4       |
| Monthly | 12        |
| Weekly  | 52        | 
"
cat(tabl)
```

```{r}
autoplot(melsyd[,"Economy.Class"]) +
  ggtitle("Economy class passengers: Melbourne-Sydney") +
  xlab("Year") +
  ylab("Thousands")
```

Features: 

- no passengers carried in 1989 due to a dispute
- reduced load due to a trial in 1992
- ...

These features have to be detected and accounted for.

```{r}
autoplot(a10) +
  ggtitle("Antidiabetic drug sales") +
  ylab("$ million") +
  xlab("Year")

```

- clear trend
- seasonality (increasing in magnitude)

#### Time series patterns

- Trend: long-term increase or decrease in the data. Might also be changing over time.
- Seasonal: Pattern with fixed and known frequency
- Cyclic: Rises and falls which are not of a fixed frequency

```{r}
# visualize seasonal pattern
ggseasonplot(a10, year.labels = TRUE, year.labels.left=TRUE) + 
  ylab("$ million") + 
  ggtitle("Seasonal plot: antidiabetic drug sales")

# alternatively
ggseasonplot(a10, polar=TRUE) +
  ylab("$ million") +
  ggtitle("Polar seasonal plot: antidiabetic drug sales")

# emphasise seasonal patterns
ggsubseriesplot(a10) +
  ylab("$ million") +
  ggtitle("Seasonal subseries plot: antidiabetic drug sales")

```

#### Scatterplots

```{r}
autoplot(elecdemand[,c("Demand","Temperature")], facets=TRUE) +
  xlab("Year: 2014") + ylab("") +
  ggtitle("Half-hourly electricity demand: Victoria, Australia")

# relationship demand and temperature
qplot(Temperature, Demand, data=as.data.frame(elecdemand)) +
  ylab("Demand (GW)") + xlab("Temperature (Celsius)")

```

Also, it is common to compute **correlations**, i.e. the correlation coefficient r. The correlation coefficient measures only the linear relationship.

### Scatterplot matrics

```{r}
autoplot(visnights[,1:5], facets=TRUE) +
  ylab("Number of visitor nights each quarter (millions)")

# check all relationships
#GGally::ggpairs(as.data.frame(visnights[,1:5]))
```

#### Lagged plots

```{r}
beer2 <- window(ausbeer, start=1992)
gglagplot(beer2)
```

#### Autocorrelation
Correlation measures the extent of a linear relationship between two variables, atuocorrealtion measures the linear realtionship between lagged values of a time series. 

```{r}
# correlogramm
ggAcf(beer2)

```

Effects on ACF:

- Trend: ACF for small lags tend to be large and positive. ACF tend to have positive values that slowly decrease as the lags increas
- Seasonal: ACF larger for seasonal lags (multiples of seasonal frequency)
- Trend and seasonal: Combination of these effects

```{r}
aelec <- window(elec, start=1980)
autoplot(aelec) + 
  xlab("Year") +
  ylab("GWh")

ggAcf(aelec, lag=48)

```


### White noise
Time Series that show no autocorrelation are called white noise. Sum of white noise realizations is called random walk

```{r}
set.seed(30)
y <- ts(rnorm(50))
autoplot(y) +
  ggtitle("White Noise")

ggAcf(y)

```

#### Exercises

```{r}
# gold
autoplot(gold)
frequency(gold)
gold[which.max(gold)]

# read in the data
tute1 <- read.csv("data/tute1.csv", header=TRUE)

# convert to time series 
my_ts <- ts(tute1[,-1], start=1981, frequency=4)

autoplot(my_ts, facets=T)

# retail
retail <- readxl::read_excel("data/retail.xlsx", skip=1)
my_ts2 <- ts(retail[,"A3349873A"],
             frequency=12,
             start=c(1982,4))

autoplot(my_ts2)
ggseasonplot(my_ts2)
ggseasonplot(my_ts2, polar = TRUE)
ggsubseriesplot(my_ts2)
gglagplot(my_ts2)
ggAcf(my_ts2)

dj_d <- diff(dj)
autoplot(dj_d)
ggAcf(dj_d)
```

# Chapter 3: The forecaster's toolbox

#### Average method
Forecats of all future values are equal to the average of historical data

$$ \hat{y}_{T+h|T} = \bar{y} $$

```{r}
# fc for white noise
fc_y <- meanf(y, h=10)
autoplot(fc_y)
```

#### Naive method
forecast is simply the value of the last observation

$$ \hat{y}_{T+h|T} = y_T $$
This method works surprisingly well for many economic and financial time series

```{r}
fc2_y <- naive(y,h=10)
autoplot(fc2_y)
```

#### Seasonal naive method
Set each forecast to be equal to the last observed value from the same season 

$$\hat{y}_{T+h|T} = y_{T+h-m (k+1)}$$
where m is the seasonal period, and k is the integer part of $(h-1)/m$

```{r}
fc_my_ts2 <- snaive(my_ts2, h=10)
```

#### Drift method
Variation of naive method to allow the forecast to increase or decrease over time. Equivalent to drawing a line between the first and last observation, and extrapolating it into the future


#### Examples
```{r}
# Plot some forecasts
autoplot(beer2) +
  autolayer(meanf(beer2, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(beer2, h=11),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(beer2, h=11),
    series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))

# non-seasonal methods
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```

Often, these forecasting methods serve as benchmarks rather than the method of choice.

## Transformations and adjustments
Use transformations and adjustmens to simplify the patterns in the historical data by removing known sources of variation or making the pattern more consistens across the whole data set. 

#### Calendar adjustments

```{r}
# Problem: different days of month
dframe <- cbind(Monthly=milk,
                DailyAverage = milk/monthdays(milk))
autoplot(dframe, facet=T) +
  xlab("Years") + 
  ylab("Pounds") +
  ggtitle("Milk production per cow")

```

#### Population adjustments
Adjust data to give per-capita data. Easier to interpret and compare and eliminates the effect of population changes.

#### Inflation adjustments
Use a price index to account for changes of purchasing power. With price index $z_t$ and $y_t$ denotes the original house price in year t,

$$ x_t = y_t / z_t \cdot z_{year} $$

A common price index is the Consumer Price Index (CPI).

#### Mathematical transformations
Variation that increases or decreases with the level of that series can be approached with transformations like the log-transformation. Other transformations: 

- power transformations
- Box-Cox transformations

```{r}
lambda <- BoxCox.lambda(elec)
autoplot(BoxCox(elec, lambda))
```

Afterwards, the inverse of the Box.Cox transformation (`InvBoxCox()`) should be applied to obtain the initial scale. 

#### Bias adjustments
Back-transformed point forecast such as Box-Cox transformation will usually be the median of the forecast distritbution (assuming that the distibribution on the transformed space is symmetric). If you want to add up different forecasts and combine them this should be adjusted (bias-adjusted). Medians do not add up, whereas means do.

```{r}
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation") +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
```

If you want your forecats to be means rather than medians, use the argument `biasadj=TRUE` when you select your Box-Cox transformation parameter.

## Residual diagnostics
Residuals are equal to the difference between the observations and the corresponding fitted values: 

$$e_t = y_t - \hat{y}_t$$
They are useful to check whether a model has captured the information in the data (only white noise should be left in residuals). A good forecasting method yields residuals with the following properties:

- Residuals are uncorrelated. 
- Residuals have zero mean. 

In addition, it is useful but not necessary that: 

- Residuals have constant variance
- Residuals are normally distributed.

The latter properties make the calculation of prediction intervals easier. Alternative approach to obtain prediction intervals may be necessary. 

```{r}
autoplot(goog200) +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google Stock (daily ending 6 December 2013)")

# residuals 
res <- residuals(naive(goog200))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")

# Histogram
gghistogram(res) + ggtitle("Histogram of residuals")

ggAcf(res) + ggtitle("ACF of residuals")
```

#### Portmanteau test for autocorrelation
A test for a group of autocorrelations is called a portmanteau test. Common tests are the **Box-Pierce Test** and the **Ljung-Box test**. 
These are tesing whether the first h autocorrelations are significantly different from what would be expected from a white nois process. 

Not significant results mean they are not distinguishable from a white noise process. Use the function `checkresiduals()`to produce a time plot of the residuals, ACF, histogram and a Ljung-Box test altogether.

## Evaluating forecast accuracy





