{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python (Chollet)\n",
    "## Chapter 5: Deep learning for computer vision\n",
    "\n",
    "- Convolutional neural networks (convnets)\n",
    "- apply convnets to image-classification problems (in particular involving small training datasets. \n",
    "- convnets are a building block of LSTM and other advanced model used in time series analysis\n",
    "\n",
    "### Introduction to convnets\n",
    "\n",
    "- Practical example before theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers \n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "# add layers\n",
    "model.add(layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Alternating `Conv2D` and `MaxPooling2`\n",
    " - [Convolutional layers](https://keras.io/layers/convolutional/): Layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    " - [Pooling layers](https://keras.io/layers/pooling/): Max pooling operation for spatial data \n",
    "- Inputs are tensors of shape `(image_height, image_width, image_channels)`, here `(28, 28, 1)`\n",
    "- Feed the last output tensor `(3,3,64)`into a densely connected classifier network after flattening the 3D outputs to 1D.\n",
    "- Adding a classifier using `Dense`:\n",
    " - [Dense layer](https://keras.io/layers/core/): A regular densely-connected NN layer\n",
    "\n",
    "\n",
    "![convnet](../images/convnet.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a classifier\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- 10-way classification and softmax activation\n",
    "- flattened to 576 $(3*3*64)$ variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Train this simple convnet on the MNIST digits.\n",
    " - `datasets`: [link](https://keras.io/datasets/)\n",
    " - MNIST: 60,000 28x28 images of 10 digits, along with test set of 10,000 images.\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    " - `to_categorical`([Link](https://keras.io/utils/)) converts a class vector (integers) to binary class matrix. Returns a binary matrix representation of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train images: (60000, 28, 28)\n",
      "Shape test_images: (10000, 28, 28)\n",
      "Shape train images: (60000, 28, 28, 1)\n",
      "Shape test_images: (10000, 28, 28, 1)\n",
      "Shape train labels: (60000, 10)\n",
      "Shape test labels: (10000, 10)\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0166 - acc: 0.9949\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0130 - acc: 0.9961\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0111 - acc: 0.9968\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.0087 - acc: 0.9972\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0081 - acc: 0.9977\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "# load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(\"Shape train images:\", train_images.shape)\n",
    "print(\"Shape test_images:\", test_images.shape)\n",
    "\n",
    "# reshape train and normalize\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "# reshape test and normalize\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "print(\"Shape train images:\", train_images.shape)\n",
    "print(\"Shape test_images:\", test_images.shape)\n",
    "\n",
    "# one-hot encoding labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(\"Shape train labels:\", train_labels.shape)\n",
    "print(\"Shape test labels:\", test_labels.shape)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", \n",
    "             loss=\"categorical_crossentropy\", \n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "fit1 = model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# evaluation\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 99.17%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \"+ str(test_acc*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Even this simple convnet achieved accuracy of more than 90%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution operation\n",
    "\n",
    "- Main difference to densely connected layer: `Dense` layers learn **global patterns** in their input space, whereas convolution layers learn **local patterns**.\n",
    "\n",
    "This key characteristic gives two interesting properties:\n",
    "\n",
    "- **Patterns are translation invariant:** Certain patterns can be found everywhere in the picture (does not depend on whether it was in the lower right-corner or at a different place). Makes convnets very efficient\n",
    " - Fewer training samples are needed to learn representations that have generalization power.\n",
    " - *Visual world is fundamentally translation invariant*\n",
    "- **They learn spatial hierarchies of patterns:** First convolition layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layer, and so on. \n",
    " - Allows convnets to efficiently learn increasingly complex and abstract visual concepts\n",
    " - *Visual world is fundamentally spatially hierarchical*\n",
    " \n",
    "Convolutions operate over 3D tensors (feature maps):\n",
    "\n",
    "- 2 spatial axes (*height* and *width*)\n",
    "- 1 depth axis (also called the *channels* axis)\n",
    "\n",
    "An RGB image has three color channels and has therefore *depth* of 3, whereas a grayscale image like the mnist images have a *depth* of 1.\n",
    "\n",
    "Convolution operation extracts patches from the input feature map (receptive field of the input with the same size as the filter) and applies the filter/kernel which yields the output feature map. \n",
    "\n",
    "<img src=\"../images/input_filter.png\" width=70%>\n",
    "\n",
    "<img src=\"../images/feature_map.png\" width=70%>\n",
    "\n",
    "<img src=\"../images/conv.gif\" width=\"85%\">\n",
    "\n",
    "Here, first convolution layer takes a feature map of size `(28, 28, 1)` and outputs a feature map of size `(26, 26, 32).`. It computes 32 filters over its input containing a 26x26 grid of values (*response map*) of the filter over the input. \n",
    "\n",
    "- Feature map: Every dimension in the depth axis is a feature (filter), and the 2D tensor `output[:, :, n]` is the 2D spatial map of the response of this filter over the input. \n",
    "\n",
    "<img src=\"../images/concept_response_map.png\">\n",
    "\n",
    "Convolutions have two key parameters:\n",
    "\n",
    "- **Size of the patches extraced from the inputs:** Typically, these are 3x3 or 5x5 (for mnist, we have used 3x3 which can be seen from the summary of the model).\n",
    "- **Depth of the output feature map:** Number of filters computed by the convolution (32 and 64).\n",
    "\n",
    "These are the first arguments passed to the `Conv2D` layers: \n",
    "\n",
    "```python\n",
    "Conv2D(output_depth, (window_height, window_width))\n",
    "```\n",
    "\n",
    "Convolution steps: \n",
    "\n",
    "- Sliding these tiny 3x3 (or 5x5) windows over the 3D input feature map\n",
    "- stopping at every possible location\n",
    "- extracting 3D patch of surrounding features with shape `(window_height, window_width, input_depth)` \n",
    " - Each patch is then transformed (via tensor product with same learned weight matrix, called *convolution kernel*) into a 1D vector of shape `(output_depth, )`\n",
    "- All vectors are then spatially reassembled into a 3D output map of shape `(height, width, output_depth)`. \n",
    "\n",
    "<img src=\"../images/fig_5_4.png\" width=\"60%\">\n",
    "\n",
    "- Output width and heigt may differ from the input width and height due to\n",
    " - **Border effects**, which can be countered by **padding** the input feature map\n",
    " - The use of **strides**\n",
    " \n",
    "(see next section)\n",
    "\n",
    "### Border effects and padding\n",
    "\n",
    "Using a 5x5 feature map and a 3x3 window (filter/kernel?): \n",
    "\n",
    "<img src=\"../images/fig_5_5.png\" width=\"60%\">\n",
    "\n",
    "There are 9 positions around which we can center a 3x3 window, forming a 3x3 grid. This leads to an output feature map of 3x3. The output map shrinks by two tiles alongside each dimension: \n",
    "\n",
    "- 28x28 input features become 26x26 after the first convolition layer, ...\n",
    "\n",
    "Here comes **padding** into the game. Using padding, we can get the same spatial dimensions as the input. \n",
    "\n",
    "> Padding adds appropriate number of rows and columnes on each side to so it is possible to fit center convolution windows around every input tile. \n",
    "\n",
    "Using a 3x3 window (filter/kernel), we add one column on the right and left as well as one row at the top and the bottom. In Keras' [`Conv2D`](https://keras.io/layers/convolutional/) layers padding is configurable via the `padding` argument, which takes two values:\n",
    "\n",
    "- `padding=\"valid\"`: No padding (only valid window locations will be used). This is the default.\n",
    "- `padding=\"same\"`: Pad in a way that output have the same width and height as the input.   \n",
    "\n",
    "\n",
    "### Convolution strides\n",
    "\n",
    "So far, assuming center tiles of the convolution window are contiguous. \n",
    "\n",
    "> Distance between two successive windows is a parameter of the convolution, called stride\n",
    "\n",
    "Patches extracted by a 3x3 convolution with stride 2 over a 5x5 input (no padding). \n",
    "\n",
    "<img src=\"../images/fig_5_7.png\" width=\"70%\">\n",
    "\n",
    "- stride equal 2 means width and height of the feature map are downsampled by a factor of 2.\n",
    "\n",
    "Strided convolutions are rarely used in practice. To downsample feature maps, instead of strides, we use the **max-pooling** operation. \n",
    "\n",
    "### The max-pooling operation\n",
    "\n",
    "From the model summary, we see that the size of the feature maps is halved after every `MaxPooling2D` layer, i.e. form 26x26 to 13x13. This is done by max pooling. \n",
    "\n",
    "> Max-pooling aggressively downsamples feature maps\n",
    "\n",
    "Max-pooling extracts windows from the input feature maps and outputs the max value of each channel. Similar to convolution, except that instead of transforming the local patches via a learned linear transformation (convolution kernel), they're transformed via a hardcoded max tensor operation. \n",
    "\n",
    "- Max pooling is usually done with 2x2 windows and stride of 2\n",
    " - leads to downsample the feature maps by a factor of 2\n",
    "- Convolution is typically done with 3x3 windows and no stride (means stride equals one)\n",
    "\n",
    "> Reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce sspatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover).\n",
    "\n",
    "Basics of convnet so far: \n",
    "\n",
    "- Feature maps\n",
    "- convolution\n",
    "- max pooling \n",
    "\n",
    "and their relation to each other.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key terms\n",
    "- *Convolution:*\n",
    "- *filter/kernel:*\n",
    "- *Feature map:* A feature map is a function which maps a data vector to feature space. Intuitively, this is done to present your learning algorithm with data that are better able to classify.  \n",
    " - i.e. a function that takes feature vectors in one space and transforms them into feature vectors in another space.\n",
    " - Input is mapped to the convolution layer (convolution filer/kernel) to produce a feature map.\n",
    "- *Response map*:\n",
    "- *Border effect* and *padding*:\n",
    "- *Convolution strides*: \n",
    "- *Max-pooling*:\n",
    " - *downsampling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a convnet from scratch on a small dataset\n",
    "\n",
    "\n",
    "\n",
    "#### Key Terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
