{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python (Chollet)\n",
    "## Chapter 6: Deep learning for text and sequences\n",
    "\n",
    "Recall that a convolutional neural net (convnet) is a stack of `Conv2D`[(link)](https://keras.io/layers/convolutional/) and `MaxPooling2D`[(link)](https://keras.io/layers/pooling/) layers. See notes on chapter 5 for more on this topic.\n",
    "\n",
    "In the area of sequence processing such as time series and text analysis, two fundamental deep-learning algorithms for sequence processing are: \n",
    "- Recurrent neural networks\n",
    "- 1D convnets ([link](https://keras.io/layers/convolutional/) )\n",
    "\n",
    "Two applications:\n",
    "\n",
    "- Sentiment analysis on IMDB dataset\n",
    "- Temperature forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text data\n",
    "\n",
    "- Text as a sequence of words. \n",
    "- Map the statistical structure of written language\n",
    "\n",
    "First step, transform the text to an input that the computer can read. \n",
    "\n",
    "> **Vectorization**: Vectorizing text is the process fo transforming text into numeric tensors.\n",
    "\n",
    "Approaches to do this: \n",
    "\n",
    "- Segment text into words, transform each word into a vector\n",
    "- Segment text into character, transform characters into a vector\n",
    "- Extract n-grams of words or characters, and transform each n-gram invo a vector. N-grams are overlapping groups of multiple consecutive words or characters. \n",
    " - E.g. a 3-gram of \"The cat did\" equals to the following sets: {\"The\", \"The cat\", \"The cat did\", \"cat did\", \"cat\", \"The did\"}\n",
    " - Groups of N (or fewer) consecutrive words that can be extraced from the sentence.\n",
    "  \n",
    "In general, units into which text is broken down are called tokens. Breaking down text into units is therefore called tokenization. \n",
    "\n",
    "### One-hot encoding of words and characters\n",
    "\n",
    "Most common and basic way to turn a token into a vector (token is there vs. token is not there encoded as 0/1 variable).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework': 10}\n",
      "\n",
      "Shape of results: (2, 10, 11)\n",
      "\n",
      "Result of one-hot encoding:\n",
      "\n",
      " [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "import numpy as np\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework\"]\n",
    "\n",
    "token_index = {}\n",
    "\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            \n",
    "        \n",
    "print(token_index)\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape= (len(samples),\n",
    "                           max_length, \n",
    "                           max(token_index.values()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1\n",
    "\n",
    "print(\"\\nShape of results:\", results.shape)\n",
    "print(\"\\nResult of one-hot encoding:\\n\\n\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, built-in utility functions are used for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "# use built-in function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework\"]\n",
    "\n",
    "# initialize and fit Tokenizer object\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left out: \n",
    "- (one-hot hashing trick)\n",
    "- (hash collisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word-embeddings\n",
    "\n",
    "- Word embeddings are dense word vectors\n",
    " - word embeddings pack more information into far fewer dimensions. \n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer can be understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. \n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000, 20)\n",
      "(25000, 20)\n"
     ]
    }
   ],
   "source": [
    "# loading IMDB data for use with an Embedding layer\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000 # words to consider as features\n",
    "maxlen = 20 # cuts off text after 20 most common words \n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test)= imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# turn lists of integers into a 2D integer tensor\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 308us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 161us/step - loss: 0.5657 - acc: 0.7426 - val_loss: 0.5467 - val_acc: 0.7204\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 6s 315us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 6s 319us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7536\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 190us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 6s 295us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 177us/step - loss: 0.3023 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 6s 313us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n",
      "25000/25000 [==============================] - 1s 29us/step\n"
     ]
    }
   ],
   "source": [
    "# embedding and classifier on the imdb data\n",
    "\n",
    "# imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "\n",
    "# flatten 3d tensor into a 2D tensor\n",
    "model.add(Flatten())\n",
    "\n",
    "# add classifier on top\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32, \n",
    "                   validation_split=0.2)\n",
    "\n",
    "# eval\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 75.58%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \"+ str(test_acc*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this model treats each word in the input sequence separately, without considering inter-word relationships and sentence structure. We can do better by adding recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take sequences into account. This is the motivation for RNN. \n",
    "\n",
    "left out: \n",
    "- Using pretrained word embeddings\n",
    "\n",
    "### Putting it all together \n",
    "\n",
    "- left out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
