{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Time Series Forecasting\n",
    "\n",
    "Recurrent neural networks (RNN) are a type of neural network that handle sequences, i.e. the relation between observations to each other.\n",
    "\n",
    "Therefore, RNN seem to be able to learn temporal contexts. In addition, there may be the hope that the explicit relationship (such as trend and seasonalities can be learned by the network without explicitly being programmed. \n",
    "\n",
    "## Time Series Forecasting\n",
    "\n",
    "- Adds the complexity of sequential order (temporal dependence between observations)\n",
    "- Requires specialized handling of the data (fitting and evaluating)\n",
    "- Adds additional structure that the model could potentially exploit (i.e. patterns like seasonality and trends)\n",
    "- Traditional time series analysis focuses on linear methods such as ARIMA models and exponential smoothing\n",
    "\n",
    "## Neural Networks for Time Series\n",
    "\n",
    "Neural networks approximate a mapping function from input variables to output variables. \n",
    " \n",
    "- Robust to noise\n",
    "- Nonlinear relationship can be captured\n",
    "\n",
    "## Recurrent Neural Networks for Time Series\n",
    "\n",
    "- Add explicit handling of order between observations when learning a mapping function from inputs to outputs. \n",
    "- Sequence as a new dimension\n",
    "- LSTM, a special kind of RNN, is able to solve many time series tasks unsolvable by feed-forward networks using fixed size time windows. \n",
    "- RNN can learn the temporal dependence from the data. \n",
    "- LSTM has the ability to learn long term correlations in a sequence\n",
    "\n",
    "> Because of this ability to learn long term correlations in a sequence, LSTM networks obviate the need for a pre-specified time window and are capable of accurately modelling complex multivaraite sequences. \n",
    "\n",
    "So there is the hope that LSTM may learn complex relationships such as trend and seasonality. In addition, practice and some research suggests removing such systematic structures to simplify the problem space (e.g. Makridakis et al, 2018). \n",
    "\n",
    "## Predictions with Sequences\n",
    "\n",
    "In general, observations in machine learning are treated equally wrt to their order to each other. This is different for sequences. \n",
    "\n",
    "- Sequences impose an explicit order on the observations.\n",
    "\n",
    "### Sequence prediction\n",
    "\n",
    "Input is an ordered sequence and the task is to predict the next value in the sequence. \n",
    "\n",
    "Examples: \n",
    "\n",
    "- Weather forecasting, Stock market prediction, Product Recommendation\n",
    "\n",
    "### Sequence classification\n",
    "\n",
    "Sequence is given, what is the class label of the sequence?\n",
    "\n",
    "Examples for classes: \n",
    "\n",
    "- Trend/no trend, seasonality/ no seasonality\n",
    "- Anomaly detection, Sentiment analysis (text is a typical example of sequences that are dealt with in deep learning)\n",
    "\n",
    "### Sequence Generation\n",
    "\n",
    "Generating a new output sequence that has the same general characteristics as the input.\n",
    "\n",
    "Examples: \n",
    "\n",
    "- Text generation, Handwriting prediction, Music generation\n",
    "\n",
    "### Sequence-to-Sequence Prediction\n",
    "\n",
    "Predicting an output sequence given an input sequence.\n",
    "\n",
    "Examples: \n",
    "\n",
    "- Multi-step Time Series Forecasting\n",
    "- Text summarization\n",
    "- Program execution\n",
    "\n",
    "After clarifying sequences, how can these problems be approached?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Long Short-Term Memory Networks\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network that respect the sequential order of a time series or other data that are ordered in this way.\n",
    "\n",
    "- Recurrent networks have an internal *state* that can represent context information\n",
    "- The *state* keeps information about past inputs for an amount of time that is not fixed but depends on its weights and the input data. \n",
    "- This network can be used to transform an input sequence into an output sequence\n",
    "\n",
    "Requirements of a RNN:\n",
    "\n",
    "- System can store information for an arbitray duration\n",
    "- System is resistant to noise\n",
    "- System paramaters can be trained (in reasonable time frame)\n",
    "\n",
    "Context in RNN: \n",
    "\n",
    "- RNN contain cycles that feed the network activations from a previous time step as inputs to the network to influence predictions at the current time step.  \n",
    "- Activations are stored internally\n",
    " - In principle, they can hold long-term temporal contextual information. \n",
    " \n",
    "### LSTMs\n",
    "\n",
    "- RNN fail to learn in the presence of time lags greater than 5-10 time steps (vanishing gradient).\n",
    "- LSTM is not affected by this problem and can learn time lags in excess of 1000 discrete time steps\n",
    "- This can be achieved by enforcing a constant error or \"constant error carrousel\" whitin special units (special cells)\n",
    "\n",
    "Problems the LSTM addresses: \n",
    "\n",
    "1. Vanishing gradient\n",
    "2. Exploding gradients\n",
    " \n",
    "Both are related to the training process of the network.\n",
    "\n",
    "Key to the success of LSTM is a specific internal structure of the units used in the model\n",
    "\n",
    "LSTM analogies: \n",
    "\n",
    "- Motivation is the error flow of existing RNNs\n",
    " - long time lags inaccessible, backpropagated error either blows up or decays exponentially\n",
    "- LSTM layers has a set of recurrently connected blocks (units which deliver information to themselves)\n",
    " - Each LSTM block contains one or more recurrently connected memory cells and three multiplicative units (input, output, forget gate) that provide continous analogues of write, read and reset operations\n",
    " - Net can only interact witht the cells via the gates. \n",
    "- Promise for any sequential processing task in which we suspect a hierarchical decomposition\n",
    "\n",
    "### Bidirectional LSTMs\n",
    "\n",
    "- Process each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer\n",
    "\n",
    "### Seq2seq LSTMs or RNN Encoder-Decoders?\n",
    "\n",
    "Idea (Ilya Sutskever, 2014):\n",
    "\n",
    "- Use one LSTM to read the input sequence, one timestep at a time.\n",
    "- Use another LSTM to extract the output sequence from that vector. \n",
    " - This second LSTM is essentially a RNN language model that is conditioned on the input sequence. \n",
    "- Does well on long sentences \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Models for Sequence Prediction with Recurrent Neural Networks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
