{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR \n",
    "\n",
    "Contents: \n",
    "\n",
    "- Overview Main paper: [Salinas et al. (2017). \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\"](https://arxiv.org/pdf/1704.04110)\n",
    "- AWS Docs: [DeepAR Forecasting Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salinas et al. (2017)\n",
    "\n",
    "### 1 - Introduction\n",
    "\n",
    "- Why TS forecasting is important\n",
    "- What's different? -> Before: Local methods; Now: Global methods\n",
    "- Extensive empirical evaluation of those who developed the algorithm. DeepAR is the go-to time series forecasting algorithm in AWS. TM: Value judgement to the disadvantage of ARIMAETS?\n",
    "- Local methods: Based on ARIMA, ES, state space or some combinations (exclude ML combinations)?\n",
    "- Global method: New type of forecasting -> Availability of hundreds or thousands of series that are related or generated by a common process...; \n",
    "- DeepAR: Autoregressive recurrent networks which learns a global model from historical data of al time series in the data set.\n",
    " - LSTM-based recurrent neural network architecture\n",
    "- Providing better forecast accuracy than previous methods?\n",
    "- Additional key advantages: \n",
    " - i) minimial feature engineering; \n",
    " - ii) probabilistic forecasts (monte carlo) -> Can be used o compute consisten quantile estimates for all sub-ranges in the prediction horizon.\n",
    " - iii) cold-start forecasting - No history at all required if similar series is added. \n",
    " - iv) Can incorporate wide range of likelihood functions\n",
    "\n",
    "\n",
    "### 2 - Related Work\n",
    "\n",
    "- Examples of forecasting individual time series include foremost ARIMA models and exponential smoothing methods.\n",
    "- Sharing information across individual time series: Can improve forecast accuracy but is difficult in practice\n",
    "- Using RNN and train these networks on all data simultaneously.\n",
    "- Main difference here: 1) Probabilistic forecasting (i.e. interested in entire distribution) 2) To obtain accurate distribution for (unbounded) count data --> Negative Binomial likelihood. \n",
    " - Negative binomial likelihood precludes us from directly applying standard data normalization techniques.\n",
    "\n",
    "\n",
    "### 3 - Model\n",
    "\n",
    "Denoting the value of time series i at time t by $z_{i,t}$, our goal is to model the conditional distribution\n",
    "$$\n",
    "P(z_{i, t_0 : T}|z_{i,1:t_0-1}, x_{i,1:T} )\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "z_{i, t_0 : T} := [z_{i,t_0}, z_{i, t_0 +1}, ..., z_{i, T}]\n",
    "$$\n",
    "\n",
    "is the future of time series until T. The difference between $t_0$ and $T$ is the forecast horizon, i.e. $z_{i,t_0 + 4}$ is equal to a forecast horizon of $h=4$. \n",
    "\n",
    "All previous realisations of the target time series until $t-1$ are denoted by\n",
    "\n",
    "$$\n",
    "z_{i,1:t_0-1} := [z_{i,1}, ..., z_{i,t_0 - 2}, z_{i,t_0 - 1}]\n",
    "$$ \n",
    "\n",
    "$t_0$ denotes the time point from which we assume $z_{i,t}$ to be unknown at prediction time, $x_{i, 1:T}$ are covariates that are assumed to be known for all time points (?also for the future?).\n",
    "\n",
    "According to this notation, the time range $[1, t_0-1]$ corresponds to the \"past\" that is used for training/conditioning, whereas the time range $[t_0, T]$ indicates the prediction range. \n",
    "\n",
    "- Time index t is relative, i.e. $t=1$ can correspond to a different actual time period for each i.\n",
    "\n",
    "The model's distribution is assumed to be \n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\theta}(z_{i,t_0:T} | z_{i,1:t_0-1}, x_{i,1:T}) \n",
    "\\end{align}\n",
    "\n",
    "- depends on own previous values until $t_0-1$ (the last known timepoint.\n",
    "- also depends on feature variables x\n",
    " - for previous timepoints\n",
    " - as well as for time points from $t_0:T$ (unknown to  end of forecast horizon) <- estimation of feature variables? \n",
    " \n",
    "The model distribution $Q_{\\theta}$ consists of a product of likelihood factors \n",
    "\n",
    "\\begin{align}\n",
    "Q_{\\theta}(z_{i,t_0:T}|z_{i,1:t_0-1},x_{i,1:T}) = \\prod_{t=t_0}^T Q_{\\theta}(z_{i,t}|z_{i,1:t-1},x_{i,1:T}) = \\prod_{t=t_0}^T \\ell( z_{i,t}|\\theta(h_{i,t}, \\Theta))\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "$$\n",
    "h_{i,t} = h(h_{i, t-1}, z_{i,t-1}, x_{i,t} , \\Theta)\n",
    "$$\n",
    "\n",
    "and h being a multi-layer recurrent neural network with LSTM cells. \n",
    "\n",
    "- Model is autoregressive (AR) in the sense that it takes in observations at the last time step $z_{i,t-1}$ as an input, as well as recurrent, i.e. previous output of the network $h_{t-1}$ is fed back as an input at the next time step. \n",
    "\n",
    "More precisely (based on the docs), DeepAR adds frequency-specific time-lags which are used as additional feature variables.\n",
    "\n",
    "Based on the frequency, the following feature variables are added:\n",
    "\n",
    "<img src=\"../images/frequency_feature_table.png\" width=\"80%\">\n",
    "\n",
    "- Likelihood $l(z_{i,t} | \\theta(h_{i,t})$ is a fixed conditional distribution\n",
    "- Parameters are given by a function $\\theta(h_{i,t}, \\Theta)$ \n",
    "- Network output: $h_{i,t}$\n",
    "\n",
    "- ?Information about (...)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR: AWS Forecasting Algorithm\n",
    "\n",
    "### What is DeepAR?\n",
    "\n",
    "DeepAR is a supervised learning algorithm for time series forecasting that uses recurrent neural networks (RNN) to produce probabilistic forecasts.\n",
    "\n",
    "### DeepAR highlights\n",
    "\n",
    "- Claims to be able to be more accurate compared to classical forecasting techniques such as ARIMA or ES.\n",
    "- Cold start forecasting: Can generate forecast for a time series with little or no existing historical data (forecasts based on the net trained with similar series)\n",
    "- Can produce points forecasts as well as probabilistic forecasts (i.e. forecast lies between X and Y with Z% probability)\n",
    "- Information sharing across series\n",
    "- DeepAR forecasting algorithm can be used with AWS \n",
    "\n",
    "For an overview of the Amazon Sagemaker, refer to the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html). \n",
    "\n",
    "### DeepAR Forecasting Algorithm\n",
    "\n",
    "The following notes are based on Amazon's documentation: [DeepAR Forecasting Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) as one of the built-in algorithms in the Amazon SageMaker.\n",
    "\n",
    "> DeepAR forecasting algorithm is a supervised learning algorithm for forecasting **scalar (one-dimensional)** time series using **recurrent neural networks (RNN)**.\n",
    "\n",
    "Traditional methods, such as ARIMA or ETS, fit a single model to each individual time series. Subsequently, this model is used to extrapolate the time series into the future. There is no information sharing between the individual time series. \n",
    "\n",
    "In case of similar or related time series (how similar do they have to be?) information sharing may be beneficial. Examples include time series grouping where individual series are related to each other like products, server loads, household electricity. \n",
    "\n",
    "For this type of application, one may benefit from training a single model jointly over all of the time series using a recurrent neural network (RNN). One example of this approach is the DeepAR algorithm. In case the dataset contains hundreds of related time series, DeepAR outperforms the standard ARIMA and ETS methods. \n",
    "\n",
    "The training of the DeepAR algorithm:\n",
    "\n",
    "- Input is one or, preferably, more target time series that hve been gnerated by the same process or similar processes\n",
    "- Based on this input, the algorithm trains a model that learns an approximation of this process/processes and uses it to predict how the target time series evolves. \n",
    "- Target time series can be optionally associated with a vector of static (time-independent) categorical features provided by the `cat` field \n",
    "- Target time series can also be associated with a vector of dynamic (time-dependent) time series provided by the `dynamic_feat` field. \n",
    "- SageMaker trains the DeepAR model by randomly sampling training examples from each target time series in the training dataset.\n",
    "- Each training example consists of a pair of adjacent context and prediction windows with fixed predefined lengths\n",
    " - Control how far in the past the network can see, use `context_length` hyperparameter.\n",
    " - Control how far in the future predictions can be made, use the `prediction_length` hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output Interface for the DeepAR Algorithm\n",
    "\n",
    "- Two supported data channels: `train`, `test`(optional)\n",
    " - `test`: data used to evaluate model after taining\n",
    "- Training and test data can be provided as `JSON Lines` format.\n",
    "- Files can be in gzip or Parquet\n",
    "\n",
    "JSON Lines:\n",
    "\n",
    "- [JSON Lines](http://jsonlines.org/): text format, also called newline-delimited JSON. Conventient format for storing structured data. \n",
    " - JSON Lines has three requirements: UTF-8 Encoding,  Each Line is a Valid JSON Value, and Line Seperator is `\\n` \n",
    " - File extension `.jsonl`\n",
    " - `gzip` or `bzip2` are recommended for saving space, resulting in `.jsonl.gz` or `jsonl.bz2` files.\n",
    "\n",
    "Parquet:\n",
    "\n",
    "- [Apache Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) is a column-oriented data storage format\n",
    "- Provides efficient data compression and encoding schemes\n",
    "\n",
    "Specifying paths: \n",
    "\n",
    "- Specify single file or a directory that contains multiple files (can be stored in subdirectories)\n",
    "- If directory is specified: DeepAR uses all files in the directory as inputs for the corresponding channel, \n",
    " - except those that start with a period (.) and those named `_SUCCESS`\n",
    " - ensures that you can directly use output folders produced by Spark jobs as input channels for DeepAR training jobs\n",
    " \n",
    "Input files:\n",
    "\n",
    "- DeepAR determines input format from the file exensions (`.json`, `json.gz`, or `parquet`) in the specified input path. \n",
    "\n",
    "Records in your input files should contain: \n",
    "\n",
    "- `start` - Start timestamp as string with the format `YYYY-MM-DD HH:MM:SS`. \n",
    "- `target` - Represents the time series. An array of floating-point values or integers. \n",
    " - Missing values can be encoded as `null` literals or `\"NaN\"` strings in JSON, or as `nan`floating-point values in Parquet. \n",
    "\n",
    "Optional inputs: \n",
    "\n",
    "- `dynamic_feat`(optional) - Vector of custom feature time series (dynamic features). \n",
    " - In the form of an array of arrays of floating-point values or integers\n",
    " - Must have the same number of inner arrays (same number of feature time series)\n",
    " - Each inner array must have the same length as the associated target value. \n",
    " - Missing values are not supported in the features\n",
    " - Example: If target time seies represents the demand of different products, an associated dynamic_feat might be a boolean time-series which indicates whether a promotion was applied (1) to the particular product or not (0). \n",
    " \n",
    "```\n",
    "{\"start\": ..., \"target\": [1, 5, 10, 2], \"dynamic_feat\": [[0, 1, 1, 0]]}\n",
    "```\n",
    "Here, there is one time series without specified start date and of length 4, one dynamic feature (one array of arrays)\n",
    "\n",
    "- `cat` (optional) - An array of categorical features that can be used to encode the groups that the record belongs to. \n",
    " - Must be encoded as a 0-based sequence of positive integers. For example, categorical domain {R,G,B} can be encoded as {0, 1, 2}. All values from each categorical domain must be represented in the training dataset.\n",
    " - Each categorical feature is embedded in a low-dimensional space whose dimensionality is controlled by `embedding_dimension`\n",
    " \n",
    "If a JSON file is used, it mus be in JSON Lines format. For example: \n",
    "\n",
    "```\n",
    "{\"start\": \"2009-11-01 00:00:00\", \"target\": [4.3, \"NaN\", 5.1, ...], \"cat\": [0, 1], \"dynamic_feat\": [[1.1, 1.2, 0.5, ...]]}\n",
    "\n",
    "{\"start\": \"2012-01-30 00:00:00\", \"target\": [1.0, -5.0, ...], \"cat\": [2, 3], \"dynamic_feat\": [[1.1, 2.05, ...]]}\n",
    " \n",
    "{\"start\": \"1999-01-30 00:00:00\", \"target\": [2.0, 1.0], \"cat\": [1, 4], \"dynamic_feat\":\n",
    " [[1.3, 0.4]]}\n",
    "``` \n",
    "\n",
    "In this example, each time series has two associated categorical features and one time series features. \n",
    "\n",
    "For Parquet: \n",
    "\n",
    "- \"start\" can be datetime type.\n",
    "- Parquet files can be compressed using gzip or snappy compression\n",
    "\n",
    "What happens if the algorithm is trained without the optional `cat` and `dynamic_feat`? \n",
    "\n",
    "- learns a \"global\" model, \n",
    " - that is a model that is agnostic to the specific identity of the target time series at inference time and is condtioned only on its shape. \n",
    "\n",
    "What if the model is conditioned on the `cat` and `dynamic_feat` feature data? \n",
    "\n",
    "- Prediction will be influenced by the character of time series with the corresponding `cat` features. \n",
    "- For example: `target` time series represents the demand of clothing items \n",
    " - Associate a two-dimensional cat vector that encodes the type of item (e.g. 0 = shoes, 1 = dress) in the first component and the color (e.g. 0 = red, 1 = blue) in the second component.\n",
    " \n",
    "```\n",
    "{\"start\": ..., \"target\": ..., \"cat\": [0, 0], ...} # red shoes\n",
    "\n",
    "{\"start\": ..., \"target\": ..., \"cat\": [0, 1], ...} # blue dress\n",
    "```\n",
    "- At inference, you can request predictions for targets with cat values that are combinations of the cat values observed in the trainind data, for example: \n",
    "\n",
    "```\n",
    "{\"start\": ..., \"target\": ..., \"cat\": [0, 1], ...} # red dress\n",
    "\n",
    "{\"start\": ..., \"target\": ..., \"cat\": [1, 1], ...} # blue dress\n",
    "```\n",
    "\n",
    "Guidelines for training data: \n",
    "\n",
    "- Start time and length can differ\n",
    "- Series must have the same \n",
    " - frequency,\n",
    " - number of categorical features\n",
    " - number of dynamic features \n",
    "- Shuffle training file wrt position of the series in the file. \n",
    "- `start` timestep is used to derive the internal features. \n",
    "- If `cat` is used: All series must have the same number of categorical features. \n",
    " - Algorithm uses `cat` and extracts the cardinality of the groups. Can be disabled (default `cardinality` is `\"auto\"`)\n",
    " - If model was trained using a cat feature, you must include it for inference\n",
    "- If trainind data contain `dynamic_feat`: Automatically used by algorithm\n",
    " - All series must have the same number of feature series \n",
    " - Time points must correspond one-to-one to the time points in `target`\n",
    " - same length of `dynamic_feat` as `target`.\n",
    " - can be disabled\n",
    " - If model was trained with `dynamic_feat`, then it must be provided for inference. In addition, each of the features has to have the length of the provided target plus `prediction_length`. You must provide the feature value in the future. \n",
    " \n",
    "Optional test channel data: \n",
    "\n",
    "- If specified, then DeepAR evaluates trained model with different accuracy metrics \n",
    "- Accuracy metrics: $RMSE$ and $wQuantileLoss$, which will be defined later in this doc.\n",
    "- Specify the length of the forecast horizon by setting the `prediction_length` hyperparameter. \n",
    "- Specify which quantiles to calculate the loss for by setting the `test_quantiles` hyperparameter. In addition to these, the average of the prescribed losses is reported as part of the training logs. \n",
    "\n",
    "For inference: \n",
    "\n",
    "- DeepAR accepts JSON format and the following fields \n",
    " - `\"instances\"`, including one or more time series in JSON Lines format\n",
    " - A name of `\"configuration\"`, which includes parameters for generating the forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Using the DeepAR Algorithm\n",
    "\n",
    "During tuning of a DeepAR model:\n",
    "\n",
    "- Split data in training and test dataset\n",
    " - To create training and test data that fit the criteria (algorithm does not see test data): Use entire dataset as a test and remov last `prediction_length` points of each time series for training\n",
    " \n",
    "- Avoid using very large values for the `prediction_length` becaust this makes the model slow and less accuracte\n",
    " - In those cases: Consider aggregating data at a higher frequence (5min instead of 1min).\n",
    "\n",
    "- Lags enable model to look further back in the time series than the value specified for `context_length`. \n",
    " - Hence, `context_length` does not need to be large\n",
    " - Recommendation: Start with `context_length` equal to `prediction_length`. \n",
    "\n",
    "- In general, it is recommended to train DeepAR on as many time series as are available.\n",
    " - Standard forecasting algorithms, such as ARIMA or ETS, might provide more accurate results on single time series as well as to a moderate number of series.\n",
    "\n",
    "> DeepAR starts to outperform the standard methods when your dataset contains hundreds of related time series. Currently, DeepAR requires that the total number of observations available across all training time series is at least 300. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Instance Recommendations for the DeepAR Algorithm \n",
    "\n",
    "Training: \n",
    "\n",
    "- DeepAR can be trained on GPU and CPU instances in both single and multi-machine settings\n",
    "- Recommend starting with a single CPU instance, i.e. `ml.c4.2xlage` or `ml.c4.4xlarge`\n",
    " - Switch to GPU instances and multiple machines only when necessary. \n",
    "\n",
    "Inference: \n",
    "\n",
    "- DeepAR supports only CPU instances. \n",
    "\n",
    "Job failures:\n",
    "\n",
    "- large values for `context_length`, `prediction_length`, `num_cells`, `num_layers` or `mini_batch_size` can create models that are too large for small instances. \n",
    "- This may also occur when running hyperparameter tuning jobs. \n",
    "- In that case, use an instance type large enough for the model tuning job and consider limiting the upper values for the critical parameters to avoid job failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the DeepAR Algorithm Works\n",
    "\n",
    "- AWS [How the DeepAR Algorithm Works](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html)\n",
    "\n",
    "During training: \n",
    "\n",
    "- accepts training dataset and optional test dataset\n",
    "- test data to evaluate the trained model.\n",
    "- Both, training and test datasetes consist of one or, preferably, more target time series\n",
    "- Each target time series can optionally be associated with a vector of feature time series and a vector of categorical features. \n",
    "\n",
    "### Example\n",
    "\n",
    "(Notation different to paper/above?)\n",
    "\n",
    "- Training set i consists of \n",
    "- a target time series $z_{i,t}$ \n",
    "- two associated feature time series, $x_{i,1,t}$ and $x_{i,2,t}$:\n",
    "\n",
    "<img src=\"../images/deepar_example.png\">\n",
    "\n",
    "DeepAR only supports feature time series that known in the future. Why? Allows to run \"what if\" scenarios, like \"what happens, if I change the price of a prodcut in some way?\n",
    " \n",
    "### How Feature Time Series Work in the DeepAR Algorithm\n",
    "\n",
    "For time-dependent patterns, such as spikes during weekends, \n",
    "\n",
    "- DeepAR automatically creates feature time series based on the frequency of the target time series.\n",
    "\n",
    "Example (weekly data): \n",
    "\n",
    "- DeepAR creates two feature time series\n",
    " - day of the month\n",
    " - day of the year\n",
    " \n",
    "<img src=\"../images/deepar_weekly_features.png\">\n",
    "\n",
    "The following table lists the derived features for the supported basic time frequencies: \n",
    "\n",
    "<img src=\"../images/frequency_feature_table.png\">\n",
    "\n",
    "### How are the models trained in DeepAR?\n",
    "\n",
    "- Random sampling: Several training examples from each of the time series in the training dataset are randomly sample. \n",
    " - Training examples: Consists of a parif of adjacent context and prediction windows with fixed predefined lengths. \n",
    "  - `context_length`: Controls how far in the past the network can see\n",
    "  - `prediction_length`: How far in the future predictions can be made. \n",
    "\n",
    "In the following figure five samples with context lengths of 12 hours and prediction lengths of 6 hours are drawn from element i. The feature time series $x_{i,1,t}$ and $u_{i,2,t}$ are ommited for brevity.\n",
    "\n",
    "<img src=\"../images/deepar_example2.png\">\n",
    "\n",
    "- DeepAR automatically feeds lagged values from the target time series (hence autoregressive). \n",
    "\n",
    "Here, in the example with hourly frequency: \n",
    "\n",
    "- Model exposes the $t_{i,t}$ values, which occured approximately one, two, and three days in the past. \n",
    "\n",
    "<img src=\"../images/deepar_lag.png\">\n",
    "\n",
    "#### Inference \n",
    "\n",
    "- takes target time series as input (independent of whether it was trained on the series or not\n",
    "- forecasts a probability distribution for the next `prediction_length` values. \n",
    "- DeepAR is trained on the entire dataset, hence forecasts takes into account patterns learned from similar time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR Hyperparameters\n",
    "\n",
    "[DeepAR hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html): \n",
    "\n",
    "- `context_length`\n",
    "- `epochs`\n",
    "- `prediction_length` - Forecast horizon\n",
    "- `time_freq` - Time frequency\n",
    " - required to select appropriate data features and lags\n",
    " - M: monthly; W: weekly, D: daily, H: hourly; min: every minute\n",
    "- `cardinality` - Array specifying the number of categories (groups) if categorical features (`cat`) are used\n",
    "- `dropout_rate` - The dropout rate to use during training. Zoneout regularization is used. \n",
    "- `early_stopping_patience` - Training stops when no prograss is made within specified number of epochs. Model with lowest loss is returned as final model. \n",
    "- `embedding_dimension` - Model can learn group-level time series patterns, where an embedding vector of this size for each group is used. \n",
    "- `learning_rate` - Learning rate used in training. \n",
    "- `likelihood` - Distribution used for generating a probabilisitic forecast. \n",
    " - gaussian, beta, negative-binomial, student-t, deterministic-L1\n",
    "- `mini_batch_size` - The size of mini-batches used during training.\n",
    "- `num_cells` - Cells to use in each hidden layer\n",
    "- `num_dynamic_feat` - ...\n",
    "- `num_eval_sampels` - The number of samples that are used per time-series when calculating test accuracy metrics. \n",
    "- `num_layers` - The number of hidden  layers in the RNN. Defualt value: 2\n",
    "- `test_quantiles` - Quantiles for which to calculate quantile loss on the test channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune a DeepAR Model\n",
    "\n",
    "- Automatic model tuning (hyperparameter tuning) finds best version of a model by running many jobs that test a range of values\n",
    "- objective metric can be predetermined\n",
    "- More information about [automatic model tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)\n",
    "\n",
    "#### Metrics computed by the DeepAR algorithm\n",
    "\n",
    "During training, three metrics are reported. Choose one of these as the objective:\n",
    "\n",
    "RMSE: \n",
    "- `test:RMSE` - RMSE between forecast and actual target computed on the test set \n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{ \\frac{1}{nT} \\sum_{t=1}^{T} (\\hat{y}_{i,t} - y_{i,t})^2  }\n",
    "$$\n",
    "\n",
    "where $y_{i,t}$ is the true value of time series i at the time t. $\\hat{y}_{i,t}$ is the mean prediction. The sum is over all n time series in the test set and over the last T time points for each series. T corresponds to the forecast horizon (come on, why T?). \n",
    "\n",
    "- `test:mean_wQuantileLoss` - Average overall weighted quantile loss on the test set. Control which quantiles are used by setting the `test_quantiles` hyperparameter. \n",
    "- For a quantile in the range [0,1], the weighted quantile loss is defined as:\n",
    " \n",
    "$$\n",
    "wQuantileLoss_{\\tau} = 2 \\frac{\\sum_{i,t}Q_{i,t}^{(\\tau)} }{\\sum_{y,t} |y_{i,t}| }\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "  Q_{i,t}^{(\\tau)}=\\begin{cases}\n",
    "    (1-\\tau)\\: |q_{i,t}^{(\\tau)} - y_{i,t}| , & \\text{if $q_{i,t}^{(\\tau)} > y_{i,t}$}.\\\\\n",
    "    \\tau \\: |q_{i,t}^{(\\tau)} - y_{i,t}|, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$q_{i,t}^{(\\tau)}$ is the $\\tau$-quantile of the distribution that the model predicts. \n",
    "\n",
    "- `train:final_loss` - The training negative log-likelihood loss averaged over the last training epoch for the model. \n",
    "\n",
    "#### Tunable Hyperparameters for the DeepAR Algorithm\n",
    "\n",
    "After evaluating the test/training loss it may be necessary to tune some hyperparameters.\n",
    "\n",
    "The hyperparameters that have the greatest impact, listed in order from the most to least impactful, on DeepAR objective metrics are: \n",
    "\n",
    "- `epochs`\n",
    "- `context_length`\n",
    "- `mini_batch_size`\n",
    "- `learning_rate`\n",
    "- `num_cells`\n",
    "\n",
    "<img src=\"../images/deepar_hyperparameters.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR Inference Formats\n",
    "\n",
    "### DeepAR JSON Request Formats\n",
    "\n",
    "Endpoint takes the following JSON request format:\n",
    "\n",
    "- `instances` field corresponds to the time series that should be forecasted by the model.\n",
    "- Provide `cat` for each instance if model was trained with categories. Should be omitted otherwise\n",
    "- If trained with custom dynamic features, provide same number of `dynamic_feat` values for each instance\n",
    " - Each should have a length given by `length(target)` + `prediction_length`, where `prediction_length` corresponds to forecast horizon. \n",
    "\n",
    "Request: \n",
    "\n",
    "```\n",
    "{\n",
    "     \"instances\": [\n",
    "         {\n",
    "             \"start\": \"2009-11-01 00:00:00\",\n",
    "             \"target\": [4.0, 10.0, \"NaN\", 100.0, 113.0],\n",
    "             \"cat\": [0, 1],\n",
    "             \"dynamic_feat\": [[1.0, 1.1, 2.1, 0.5, 3.1, 4.1, 1.2, 5.0, ...]]\n",
    "         },\n",
    "         {\n",
    "             \"start\": \"2012-01-30\",\n",
    "             \"target\": [1.0],\n",
    "             \"cat\": [2, 1],\n",
    "             \"dynamic_feat\": [[2.0, 3.1, 4.5, 1.5, 1.8, 3.2, 0.1, 3.0, ...]]\n",
    "         },\n",
    "         {\n",
    "             \"start\": \"1999-01-30\",\n",
    "             \"target\": [2.0, 1.0],\n",
    "             \"cat\": [1, 3],\n",
    "             \"dynamic_feat\": [[1.0, 0.1, -2.5, 0.3, 2.0, -1.2, -0.1, -3.0, ...]]\n",
    "         }\n",
    "     ],\n",
    "     \"configuration\": {\n",
    "     \"num_samples\": 50,\n",
    "     \"output_types\": [\"mean\", \"quantiles\", \"samples\"],\n",
    "     \"quantiles\": [\"0.5\", \"0.9\"]\n",
    "     }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "`configuration` is optional: \n",
    " \n",
    "- `configuration.num_samples` - sets number of sample paths that the model generates to estimate the mean and quantiles. \n",
    "- `configuration.output_types`- describes information that will be returned in the request. \n",
    " - valid values are: \"mean\", \"quantiles, \"samples\"\n",
    " - \"quantiles\" returns each of the quantile values in `configuration.quantiles` \n",
    " - Specifying `samples` returns the raw samples used to calculate the other outputs. \n",
    " \n",
    "### DeepAR JSON Response Formats\n",
    "\n",
    "Response , where `[...]` are arrays of numbers: \n",
    "\n",
    "```\n",
    "{\n",
    "     \"predictions\": [\n",
    "         {\n",
    "             \"quantiles\": {\n",
    "                 \"0.9\": [...],\n",
    "                 \"0.5\": [...]\n",
    "             },\n",
    "             \"samples\": [...],\n",
    "             \"mean\": [...]\n",
    "             },\n",
    "             {\n",
    "                 \"quantiles\": {\n",
    "                     \"0.9\": [...],\n",
    "                     \"0.5\": [...]\n",
    "                 },\n",
    "                 \"samples\": [...],\n",
    "                 \"mean\": [...]\n",
    "             },\n",
    "             {\n",
    "                 \"quantiles\": {\n",
    "                     \"0.9\": [...],\n",
    "                     \"0.5\": [...]\n",
    "                 },\n",
    "                 \"samples\": [...],\n",
    "                 \"mean\": [...]\n",
    "             }\n",
    "     ]\n",
    "}\n",
    "```\n",
    "\n",
    "- Response timeout of 60 seconds\n",
    "- When passing multiple time series in a single request, forecasts are generated sequentially\n",
    "- Forecast for each series typically takes about 300 to 1000 milliseconds, depending on model size, etc.\n",
    "- Passing too many series may cause tieouts. \n",
    "- Better: Send fewer series per request and send more requests (HOW?)\n",
    " - Because DeepAR uses multiple workers per instance, this achieves much higher throughpout by sending multiple requests in parallel. \n",
    "- DeepAR uses one worker per CPU for inference (defualt)\n",
    "- Number of workers for inference can be overwritten when calling the SageMaker [CreateModel](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform with the DeepAR Algorithm\n",
    "\n",
    "- Getting inferences by using batch transform from data using the JSON Lines format. \n",
    "- Each record is represented on a signle line as a JSON object, and lines are separated by newline characters. \n",
    "\n",
    "For example: \n",
    "\n",
    "```\n",
    "{\"start\": \"2009-11-01 00:00:00\", \"target\": [4.3, \"NaN\", 5.1, ...], \"cat\": [0, 1], \"dynamic_feat\": [[1.1, 1.2, 0.5, ..]]}\n",
    "\n",
    "{\"start\": \"2012-01-30 00:00:00\", \"target\": [1.0, -5.0, ...], \"cat\": [2, 3], \"dynamic_feat\": [[1.1, 2.05, ...]]}\n",
    "\n",
    "{\"start\": \"1999-01-30 00:00:00\", \"target\": [2.0, 1.0], \"cat\": [1, 4], \"dynamic_feat\": [[1.3, 0.4]]}\n",
    "```\n",
    "\n",
    "Note: \n",
    "\n",
    "- Set `BatchStrategy`value to `SingleRecord` and \n",
    "- set the `SplitType` value in the `TransformInput` configuration to `Line`\n",
    "- default values currently cause runtime failures. \n",
    "\n",
    "Configuration field: \n",
    "\n",
    "- Set once for the entire batch inference job using environment variable named `DEEPAR_INFERENCE_CONFIG`\n",
    " - Value can be passed when model is created by calling `CreateTransformJob` API. \n",
    " \n",
    "If `DEEPAR_INFERENCE_CONFIG` is missing in the container environment, the inference container uses the following default: \n",
    " \n",
    "```\n",
    "{\n",
    " \"num_samples\": 100,\n",
    " \"output_types\": [\"mean\", \"quantiles\"],\n",
    " \"quantiles\": [\"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", \"0.6\", \"0.7\", \"0.8\", \"0.9\"]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "- Output is also in JSON Lines format. Predictions are encoded as objects identical to the ones returned by responses in online inference mode\n",
    "\n",
    "For example: \n",
    "\n",
    "```\n",
    "{ \"quantiles\": { \"0.1\": [...], \"0.2\": [...] }, \"samples\": [...], \"mean\": [...] }\n",
    "``` \n",
    "\n",
    "In `TransformInput` configuration of the `CreateTransformJob` request: \n",
    "\n",
    "- Set `AssembleWith` value to `Line`, as the default `None` concatenates all JSON objects on the same line. \n",
    "\n",
    "For example, SageMaker `CreateTransformJob` request for a DeepAR job with a custom `DEEPAR_INFERENCE_CONFIG`: \n",
    "\n",
    "```\n",
    "{\n",
    "     \"BatchStrategy\": \"SingleRecord\",\n",
    "     \"Environment\": {\n",
    "         \"DEEPAR_INFERENCE_CONFIG\" : \"{ \\\"num_samples\\\": 200, \\\"output_types\\\": [\\\"mean\\\"] }\",\n",
    "         ...\n",
    "    },\n",
    "     \"TransformInput\": {\n",
    "     \"SplitType\": \"Line\",\n",
    "     ...\n",
    "     },\n",
    "     \"TransformOutput\": {\n",
    "         \"AssembleWith\": \"Line\",\n",
    "         ...\n",
    "     },\n",
    "     ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook DeepAR - 01 - Theory.ipynb to html',\n",
       " '[NbConvertApp] Writing 284541 bytes to DeepAR - 01 - Theory.html']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!jupyter nbconvert \"DeepAR - 01 - Theory\".ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
