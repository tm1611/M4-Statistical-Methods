---
title: "M4Comp Benchmark Methods Part 1: Application to one Time Series"
author: "Timo Meiendresch"
date: "05/06/2019"
knit: (function(input_file, encoding) {
  out_dir <- 'html_outputs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'M4Comp_Benchmarks1.html'))})
---

# Introduction 

The main purpose of this document is to introduce the benchmark methods used in the M4 Competition. Before going over them we need some methods and functions from previous parts. 

### Load data and libraries

```{r, message=FALSE, echo=FALSE}
rm(list=ls())
graphics.off()
```

```{r, message=FALSE}
# standard libraries
library(forecast)
library(ggplot2)
library(M4comp2018)
```

```{r}
# load data
data(M4)
names(M4[[1]])
```

```{r}
# check ts 40773
train <- M4[[40773]]$x
test <- M4[[40773]]$xx

# clear-memory-except
rm(list=setdiff(ls(), ls()[grep(pattern=c("test|train"), x=ls())] ) )

# plot
autoplot(train) +
  autolayer(test, series="test") +
  xlab("Year") + ylab("value") +
  ggtitle("Time series #40773")
```

### Load and test functions 
Previously introduced functions have been saved in the R-Script `my_utils.R` and can be loaded using the function `source("my_utils.R")` if they are both in the same functions, otherwise the folder has to be set relative to the RMd document. 

```{r}
# load utilities
source("../src/my_utils.R")
```

```{r} 
# testing the functions
fh <- give_fh(train)
fh
SeasonalityTest(train, frequency(train))
```

```{r}
head(seas_adj(train, give_fh(train) )$des_input)

head(seas_adj(train, give_fh(train))$SIout)
```

```{r}
# naive / seasonal naive forecasts for sMAPE/MASE
f1 <- naive(train, h=fh)$mean
f2 <- snaive(train, h=fh)$mean

# Accuracy measures f2
cal_MASE(insample = train, outsample = test, forecasts = f1)
cal_sMAPE(outsample = test, forecasts = f1)

# Accuracy measures f2
cal_MASE(insample = train, outsample = test, forecasts = f2)
cal_sMAPE(outsample = test, forecasts = f2)

# plot forecasts (zoomed in)
autoplot(window(train, start=c(2009))) + 
  autolayer(f1, series="Naive") +
  autolayer(f2, series="Seasonal Naive") +
  autolayer(test, series="test") +
  guides(col=guide_legend(title="Series")) +
  ggtitle("Comparing Naive methods") + 
  xlab("Year") + ylab("Value")
```

### Theta method
The theta method performed surprisingly well in the M3 competition and is therefore included as a benchmark method. Since its original description in 2000, there have been various modifications to this series like the optimised and dynamic theta method. 

```{r}
Theta.classic <- function(input, fh){
  #Used to estimate Theta classic
  
  #Set parameters
  wses <- wlrl<-0.5 ; theta <- 2
  #Estimate theta line (0)
  observations <- length(input)
  xt <- c(1:observations)
  xf <- c((observations+1):(observations+fh))
  train <- data.frame(input=input, xt=xt)
  test <- data.frame(xt = xf)
  
  estimate <- lm(input ~ poly(xt, 1, raw=TRUE))
  thetaline0In <- as.numeric(predict(estimate))
  thetaline0Out <- as.numeric(predict(estimate,test))
  
  #Estimate theta line (2)
  thetalineT <- theta*input+(1-theta)*thetaline0In
  sesmodel <- ses(thetalineT, h=fh)
  thetaline2In <- sesmodel$fitted
  thetaline2Out <- sesmodel$mean
  
  #Theta forecasts
  forecastsIn <- (thetaline2In*wses)+(thetaline0In*wlrl)
  forecastsOut <- (thetaline2Out*wses)+(thetaline0Out*wlrl)
  
  #Zero forecasts become positive
  for (i in 1:length(forecastsOut)){
    if (forecastsOut[i]<0){ forecastsOut[i]<-0 }
  }
  
  output=list(fitted = forecastsIn, mean = forecastsOut,
              fitted0 = thetaline0In, mean0 = thetaline0Out,
              fitted2 = thetaline2In, mean2 = thetaline2Out)
  
  return(output)
}
```

```{r}
# Theta method from M4
theta_classic <- Theta.classic(seas_adj(train, give_fh(train) )$des_input, fh)$mean*seas_adj(train, give_fh(train))$SIout

# vs built-in theta method
theta_built_in <- thetaf(train, h=give_fh(train))$mean

# comparison plot
autoplot(theta_classic) +
  autolayer(theta_built_in, series="thetaf()")
```

Both functions seem to yield the equivalent result. For simplicity, we'll use the built-in function `thetaf()` from here on. 

# Benchmark methods 

The M4 competition includes 8 benchmark methods. This document follows the notation enumerating them from `f1` to `f8`. `h` indicates the frequency-specific forecast horizon which is given by `give_fh(input)`. 

In general, `input` indicates the time series in the initial form and `des_input` indicates the deseasonalized version of the series using `seas_adj(input)$des_input`. 

`SIout` gives the seasonal component which is added to the deseasonalized forecast and can be obtained using `seas_adj(input)$SIout`:

- **Naive method** `(f1)`: Naive method using `naive(input, h=fh)$mean`. Forecast is simply the value of the last observation, 
$$
\hat{y}_{t+1} = y_{t}
$$

- **Seasonal Naive** `(f2)`: Seasonal naive method using `snaive(input, h=fh)$mean`. Each forecast to be equal to the last observed value from the same season
$$
\hat{y}_{T+h|T} = y_{T+h-m (k+1)}
$$
- **Naive2** `(f3)`: Naive method applied on deseasonalized input using `naive(des_input, h=fh)$mean`. Used for scaling the sMAPE error. 
- **Simple exponential smoothing** (`f4`): SES forecasts are calculated using weighted averages, where the weights decrease exponentially (`ses(des_input, h=fh)$mean`)
$$
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+h|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
$$
- **Holt's method** `(f5)`: Extension of SES to include a trend
$$
\begin{align*}
  \text{Forecast equation}&& \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
  \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  \text{Trend equation}   && b_{t}    &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1},
\end{align*}
$$
- **Holt's method (damped trend)** `(f6)`: Constant trend often leads to overforecasting. Holt's method is modified by adding a parameter that "dampens" the trend
$$
\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
  \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.
\end{align*}
$$
- **Theta method** `(f7)`: Description in Assimakopoulos and Nikolopoulos (2000). Hyndman and Billah (2001) showed that this method yields forecasts equivalently to simple exponential smoothing with drift. 
- **Combination method**: Simple combination of SES, Holt's method and damped Holt's method, i.e. `Comb=(f4+f5+f6)/3`. 

## Individual Application 

In this section we'll apply the benchmark methods to an individual time series that we've already used for naive and seasonal naive forecats. In addition, we calculate the respective accuracy measures **sMAPE**, **MASE** (requires naive2) and, finally, the overall weighted average **OWA**.

```{r}
benchmarks <- function(input, fh=give_fh(input)){ 
  des_input <- seas_adj(input, fh)$des_input
  SIout <- seas_adj(input, fh)$SIout

  f1 <- naive(input, h=fh)$mean # naive
  f2 <- snaive(input, h=fh)$mean # snaive
  f3 <- naive(des_input, h=fh)$mean*SIout # naive2
  f4 <- ses(des_input, h=fh)$mean*SIout # ses
  f5 <- holt(des_input, h=fh, damped=FALSE)$mean*SIout # holt
  f6 <- holt(des_input, h=fh, damped=TRUE)$mean*SIout # damped holt
  f7 <- thetaf(input, h=fh)$mean
  f8 <- (f4+f5+f6)/3 # comb
  
  output <- list(f1=f1, f2=f2, f3=f3, f4=f4, f5=f5, f6=f6, f7=f7, f8=f8)
  return(output)
}

benchmarks_train <- benchmarks(train)
str(benchmarks_train)

Names_benchmarks <- c("Naive", "sNaive", "Naive2", "SES", "Holt", "Damped","Theta","Comb")

names(benchmarks_train) <- Names_benchmarks
```

Next step is to evaluate these benchmark forecasts. For each individual forecast this can be done in the following way:  

```{r}
# sMAPE: Comb
sMAPE_Theta <- cal_sMAPE(outsample = test, forecasts = benchmarks_train$Theta)
sMAPE_Theta

# MASE: 
MASE_Theta <- cal_MASE(insample = train, outsample = test, forecasts = benchmarks_train$Theta)
MASE_Theta
```

Next step in the progression is to calculate accuracy measures for all benchmark forecasts

```{r}
Total_MASE <- rep(NA, length(benchmarks_train))
Total_sMAPE <- rep(NA, length(benchmarks_train))

for (i in 1:length(benchmarks_train)){
  Total_MASE[i] <- cal_MASE(insample = train, outsample = test, forecasts = benchmarks_train[[i]])

  Total_sMAPE[i] <- cal_sMAPE(outsample = test, forecasts = benchmarks_train[[i]])
}

df <- data.frame(Total_MASE, Total_sMAPE, row.names = Names_benchmarks)

# relative MASE
df$Rel_MASE <- df[,"Total_MASE"] / df[,"Total_MASE"][3]

# relative sMAPE
df$Rel_sMAPE <- df[,"Total_sMAPE"] / df[,"Total_sMAPE"][3]

# Overall weighted average (OWA) 
df$OWA <- (df[,"Rel_MASE"] + df[,"Rel_sMAPE"])/2
```

This yields the following results table for the benchmarking methods on one time series. 

```{r, echo=FALSE, results="asis"}
library(knitr)
kable(round(df,4), caption="Results table for series 40773")
``` 

Next steps: 

- Automate this procedure so it can be applied to specific subsets of the M4 Competition data and finally to the entire M4 Data.



